"""
================================================================================
INTERMEDIATE 01: Comparing Popular Optimizers (SGD, Adam, RMSprop, AdamW)
================================================================================

WHAT YOU'LL LEARN:
- How different optimizers work
- When to use each optimizer
- Adam vs AdamW
- Momentum and adaptive learning rates
- Practical comparison on a real problem

PREREQUISITES:
- Complete all beginner tutorials
- Understand basic optimization concepts

TIME TO COMPLETE: ~25 minutes
================================================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib
matplotlib.use('Agg')  # For non-interactive backend
import matplotlib.pyplot as plt

print("=" * 80)
print("COMPARING POPULAR OPTIMIZERS")
print("=" * 80)

# ============================================================================
# SECTION 1: Overview of Optimization Algorithms
# ============================================================================
print("\n" + "-" * 80)
print("OPTIMIZATION ALGORITHMS OVERVIEW")
print("-" * 80)

print("""
STOCHASTIC GRADIENT DESCENT (SGD):
  ‚Ä¢ Simplest optimizer: param -= learning_rate √ó gradient
  ‚Ä¢ Pros: Simple, works well with momentum
  ‚Ä¢ Cons: Can be slow, sensitive to learning rate
  ‚Ä¢ Best for: Well-understood problems, when you have time to tune

SGD WITH MOMENTUM:
  ‚Ä¢ Adds "velocity" to push through local minima
  ‚Ä¢ Accumulates gradients over time: v = momentum √ó v + gradient
  ‚Ä¢ Smooths out noisy gradients
  ‚Ä¢ Best for: Deep networks, noisy gradients

ADAM (Adaptive Moment Estimation):
  ‚Ä¢ Adapts learning rate for each parameter
  ‚Ä¢ Combines momentum + RMSprop
  ‚Ä¢ Pros: Works well out-of-the-box, fast convergence
  ‚Ä¢ Cons: Can generalize worse than SGD, higher memory usage
  ‚Ä¢ Best for: Quick prototyping, most deep learning tasks

ADAMW (Adam with Weight Decay):
  ‚Ä¢ Adam with correct weight decay implementation
  ‚Ä¢ Better generalization than Adam
  ‚Ä¢ Fixes Adam's weight decay bug
  ‚Ä¢ Best for: Transformers, modern architectures, when using weight decay

RMSPROP (Root Mean Square Propagation):
  ‚Ä¢ Adapts learning rate using moving average of squared gradients
  ‚Ä¢ Good for non-stationary problems
  ‚Ä¢ Best for: RNNs, online learning
""")

# ============================================================================
# SECTION 2: Create a Non-Trivial Optimization Problem
# ============================================================================
print("\n" + "-" * 80)
print("SETUP: Non-Convex Optimization Problem")
print("-" * 80)

# Set seed for reproducibility
torch.manual_seed(42)

# Generate synthetic data with non-linear pattern
n_samples = 200
X = torch.linspace(-3, 3, n_samples).reshape(-1, 1)
# Non-linear function: y = sin(x) + 0.5*x + noise
y = torch.sin(X) + 0.5 * X + torch.randn(n_samples, 1) * 0.1

print(f"Dataset: {n_samples} samples")
print(f"Task: Learn the non-linear function y = sin(x) + 0.5*x")

# ============================================================================
# SECTION 3: Define a Simple Neural Network
# ============================================================================
print("\n" + "-" * 80)
print("NEURAL NETWORK ARCHITECTURE")
print("-" * 80)

class SimpleNet(nn.Module):
    """
    Simple feedforward network with 2 hidden layers
    Input ‚Üí 20 neurons ‚Üí 20 neurons ‚Üí Output
    """
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(1, 20)
        self.fc2 = nn.Linear(20, 20)
        self.fc3 = nn.Linear(20, 1)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Print architecture
model = SimpleNet()
total_params = sum(p.numel() for p in model.parameters())
print(f"Architecture:")
print(model)
print(f"\nTotal parameters: {total_params}")

# ============================================================================
# SECTION 4: Training Function
# ============================================================================

def train_model(model, optimizer_name, optimizer, X, y, epochs=100):
    """Train model and return loss history"""
    criterion = nn.MSELoss()
    loss_history = []
    
    for epoch in range(epochs):
        # Forward pass
        y_pred = model(X)
        loss = criterion(y_pred, y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Record loss
        loss_history.append(loss.item())
        
        # Print progress
        if (epoch + 1) % 20 == 0:
            print(f"  [{optimizer_name}] Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}")
    
    return loss_history

# ============================================================================
# SECTION 5: Compare Optimizers
# ============================================================================
print("\n" + "-" * 80)
print("TRAINING WITH DIFFERENT OPTIMIZERS")
print("-" * 80)

epochs = 100
learning_rate = 0.01

# Store results for comparison
results = {}
optimizers_to_test = {}

# 1. SGD (vanilla)
print("\n1. SGD (Vanilla - No Momentum):")
model_sgd = SimpleNet()
opt_sgd = optim.SGD(model_sgd.parameters(), lr=learning_rate)
optimizers_to_test['SGD'] = (model_sgd, opt_sgd)

# 2. SGD with Momentum
print("\n2. SGD with Momentum (0.9):")
model_sgd_momentum = SimpleNet()
opt_sgd_momentum = optim.SGD(model_sgd_momentum.parameters(), 
                               lr=learning_rate, momentum=0.9)
optimizers_to_test['SGD+Momentum'] = (model_sgd_momentum, opt_sgd_momentum)

# 3. RMSprop
print("\n3. RMSprop:")
model_rmsprop = SimpleNet()
opt_rmsprop = optim.RMSprop(model_rmsprop.parameters(), lr=learning_rate)
optimizers_to_test['RMSprop'] = (model_rmsprop, opt_rmsprop)

# 4. Adam
print("\n4. Adam:")
model_adam = SimpleNet()
opt_adam = optim.Adam(model_adam.parameters(), lr=learning_rate)
optimizers_to_test['Adam'] = (model_adam, opt_adam)

# 5. AdamW
print("\n5. AdamW (Adam with Weight Decay):")
model_adamw = SimpleNet()
opt_adamw = optim.AdamW(model_adamw.parameters(), lr=learning_rate, 
                         weight_decay=0.01)
optimizers_to_test['AdamW'] = (model_adamw, opt_adamw)

# Train all models
for name, (model, optimizer) in optimizers_to_test.items():
    loss_history = train_model(model, name, optimizer, X, y, epochs)
    results[name] = loss_history

# ============================================================================
# SECTION 6: Analyze Results
# ============================================================================
print("\n" + "-" * 80)
print("RESULTS ANALYSIS")
print("-" * 80)

print("\nFinal Loss (lower is better):")
for name, loss_history in results.items():
    final_loss = loss_history[-1]
    print(f"  {name:15s}: {final_loss:.6f}")

print("\nConvergence Speed (loss after 20 epochs):")
for name, loss_history in results.items():
    early_loss = loss_history[19]  # 20th epoch (0-indexed)
    print(f"  {name:15s}: {early_loss:.6f}")

# ============================================================================
# SECTION 7: Visualize Convergence
# ============================================================================
print("\n" + "-" * 80)
print("VISUALIZATION")
print("-" * 80)

plt.figure(figsize=(12, 5))

# Plot 1: Loss curves
plt.subplot(1, 2, 1)
for name, loss_history in results.items():
    plt.plot(loss_history, label=name, linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss (MSE)', fontsize=12)
plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.yscale('log')  # Log scale to see differences better

# Plot 2: Final predictions
plt.subplot(1, 2, 2)
# Plot true data
plt.scatter(X.numpy(), y.numpy(), alpha=0.3, s=10, label='True Data', color='black')

# Plot predictions from each optimizer
colors = ['blue', 'orange', 'green', 'red', 'purple']
with torch.no_grad():
    for (name, (model, _)), color in zip(optimizers_to_test.items(), colors):
        y_pred = model(X)
        plt.plot(X.numpy(), y_pred.numpy(), label=name, linewidth=2, 
                color=color, alpha=0.7)

plt.xlabel('X', fontsize=12)
plt.ylabel('Y', fontsize=12)
plt.title('Final Predictions', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plot_path = '/home/claude/optimizer_comparison.png'
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
print(f"Plot saved to: {plot_path}")

# ============================================================================
# SECTION 8: Detailed Optimizer Explanations
# ============================================================================
print("\n" + "-" * 80)
print("DETAILED OPTIMIZER MECHANICS")
print("-" * 80)

print("""
1. SGD (STOCHASTIC GRADIENT DESCENT):
   Update: Œ∏ = Œ∏ - lr √ó ‚àáL
   
   ‚Ä¢ Simplest algorithm
   ‚Ä¢ Each step goes directly downhill
   ‚Ä¢ Can oscillate in narrow valleys
   ‚Ä¢ Very sensitive to learning rate choice

2. SGD WITH MOMENTUM:
   Velocity: v = Œ≤ √ó v + ‚àáL
   Update: Œ∏ = Œ∏ - lr √ó v
   
   ‚Ä¢ Accumulates past gradients (Œ≤ typically 0.9)
   ‚Ä¢ Builds "momentum" to push through small bumps
   ‚Ä¢ Smooths out noisy gradients
   ‚Ä¢ Can overshoot optimal values

3. RMSPROP:
   RMS: s = Œ≤ √ó s + (1-Œ≤) √ó ‚àáL¬≤
   Update: Œ∏ = Œ∏ - (lr / ‚àös) √ó ‚àáL
   
   ‚Ä¢ Adapts learning rate per parameter
   ‚Ä¢ Divides by root mean square of gradients
   ‚Ä¢ Prevents learning rate from becoming too small
   ‚Ä¢ Good for non-stationary objectives

4. ADAM (Adaptive Moment Estimation):
   Momentum: m = Œ≤‚ÇÅ √ó m + (1-Œ≤‚ÇÅ) √ó ‚àáL
   Velocity: v = Œ≤‚ÇÇ √ó v + (1-Œ≤‚ÇÇ) √ó ‚àáL¬≤
   Update: Œ∏ = Œ∏ - lr √ó (m / ‚àöv)
   
   ‚Ä¢ Combines momentum + RMSprop
   ‚Ä¢ Adapts learning rate for each parameter
   ‚Ä¢ Includes bias correction
   ‚Ä¢ Default Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999
   ‚Ä¢ Most popular optimizer in deep learning

5. ADAMW:
   Same as Adam but with corrected weight decay:
   Update: Œ∏ = Œ∏ - lr √ó (m / ‚àöv) - lr √ó Œª √ó Œ∏
   
   ‚Ä¢ Fixes Adam's weight decay implementation
   ‚Ä¢ Better regularization
   ‚Ä¢ Preferred for transformers and large models
""")

# ============================================================================
# SECTION 9: Practical Guidelines
# ============================================================================
print("\n" + "-" * 80)
print("PRACTICAL GUIDELINES: Which Optimizer to Use?")
print("-" * 80)

print("""
üéØ USE ADAM WHEN:
   ‚úì Starting a new project (good default)
   ‚úì Need fast convergence
   ‚úì Limited time for hyperparameter tuning
   ‚úì Working with RNNs or transformers
   ‚úì Example: Quick prototyping, research experiments

üìä USE SGD + MOMENTUM WHEN:
   ‚úì Final model training (often better generalization)
   ‚úì Have time to tune learning rate
   ‚úì Training CNNs (especially ResNet, VGG)
   ‚úì Want most stable long-term performance
   ‚úì Example: Production models, ImageNet training

üî¨ USE ADAMW WHEN:
   ‚úì Training transformers (BERT, GPT, etc.)
   ‚úì Using weight decay regularization
   ‚úì Need better generalization than Adam
   ‚úì Following modern best practices
   ‚úì Example: NLP models, large-scale training

‚ö° USE RMSPROP WHEN:
   ‚úì Training RNNs (historically popular)
   ‚úì Non-stationary problems
   ‚úì Online learning scenarios
   ‚úì Example: Time series, online recommendations

LEARNING RATE RECOMMENDATIONS:
‚Ä¢ SGD: 0.01 - 0.1
‚Ä¢ SGD + Momentum: 0.01 - 0.1
‚Ä¢ Adam: 0.001 (1e-3)
‚Ä¢ AdamW: 0.0001 - 0.001 (1e-4 to 1e-3)
‚Ä¢ RMSprop: 0.001

Always start with these defaults and adjust based on loss curves!
""")

# ============================================================================
# SECTION 10: Common Hyperparameters
# ============================================================================
print("\n" + "-" * 80)
print("HYPERPARAMETER REFERENCE")
print("-" * 80)

print("""
LEARNING RATE (lr):
  ‚Ä¢ Most important hyperparameter
  ‚Ä¢ Too high: Training unstable, loss explodes
  ‚Ä¢ Too low: Training too slow
  ‚Ä¢ Use learning rate schedulers to adjust during training

MOMENTUM (SGD):
  ‚Ä¢ Typical: 0.9 or 0.95
  ‚Ä¢ Higher = more smoothing but can overshoot

BETAS (Adam/AdamW):
  ‚Ä¢ Œ≤‚ÇÅ (momentum): typically 0.9
  ‚Ä¢ Œ≤‚ÇÇ (RMS): typically 0.999
  ‚Ä¢ Rarely need to change these

WEIGHT DECAY:
  ‚Ä¢ Regularization strength
  ‚Ä¢ Typical: 0.0001 to 0.01
  ‚Ä¢ Higher = more regularization
  ‚Ä¢ Use AdamW for correct implementation

EPSILON (Adam/RMSprop):
  ‚Ä¢ Numerical stability constant
  ‚Ä¢ Default: 1e-8
  ‚Ä¢ Rarely need to change
""")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("KEY TAKEAWAYS")
print("=" * 80)
print("""
1. Different optimizers make different trade-offs:
   ‚Ä¢ Speed vs stability
   ‚Ä¢ Ease of use vs final performance
   ‚Ä¢ Memory efficiency

2. Adam/AdamW are great defaults:
   ‚Ä¢ Fast convergence
   ‚Ä¢ Work well out-of-the-box
   ‚Ä¢ AdamW preferred for modern architectures

3. SGD + Momentum often achieves best final performance:
   ‚Ä¢ Requires more tuning
   ‚Ä¢ Better generalization
   ‚Ä¢ Preferred for production models

4. Always monitor loss curves:
   ‚Ä¢ Smooth decrease = good
   ‚Ä¢ Oscillations = learning rate too high
   ‚Ä¢ Plateau = learning rate too low or converged

5. No single "best" optimizer:
   ‚Ä¢ Depends on problem, architecture, data
   ‚Ä¢ Experiment and compare
   ‚Ä¢ Use learning rate schedulers for better results

NEXT STEPS:
‚Üí Experiment with different learning rates
‚Üí Learn about learning rate schedulers
‚Üí Try combining optimizers with regularization
‚Üí Study advanced optimizers (RAdam, Lookahead, etc.)
""")
print("=" * 80)
