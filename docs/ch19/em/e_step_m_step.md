# E-Step and M-Step

The Expectation-Maximization algorithm derives its name from its two alternating steps. This section provides a detailed examination of each step: the computational mechanics, closed-form derivations for common models, and practical implementation considerations.

---

## E-Step: Computing Expected Sufficient Statistics

### The Fundamental Task

The E-step computes the **posterior distribution** over latent variables given the current parameter estimate:

$$
p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})
$$

From this posterior, we extract the **expected sufficient statistics** needed for the M-step. The key insight is that we don't always need the full posterior—often only certain expectations are required.

### Posterior Computation via Bayes' Theorem

The posterior follows directly from Bayes' theorem:

$$
p(\mathbf{Z} | \mathbf{X}, \theta^{(t)}) = \frac{p(\mathbf{X} | \mathbf{Z}, \theta^{(t)}) \, p(\mathbf{Z} | \theta^{(t)})}{p(\mathbf{X} | \theta^{(t)})}
$$

The denominator is the marginal likelihood:

$$
p(\mathbf{X} | \theta^{(t)}) = \int p(\mathbf{X} | \mathbf{Z}, \theta^{(t)}) \, p(\mathbf{Z} | \theta^{(t)}) \, d\mathbf{Z}
$$

For discrete latent variables, this becomes a sum over all possible configurations.

### Expected Sufficient Statistics

For exponential family models, the E-step reduces to computing:

$$
\bar{T} = \mathbb{E}_{p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})}[T(\mathbf{X}, \mathbf{Z})]
$$

where $T(\mathbf{X}, \mathbf{Z})$ is the sufficient statistic of the complete-data distribution. This expectation is often much simpler to compute than the full posterior.

### Independence Structure

When data points are independent, the joint posterior factorizes:

$$
p(\mathbf{Z} | \mathbf{X}, \theta) = \prod_{i=1}^{N} p(z_i | \mathbf{x}_i, \theta)
$$

This reduces the E-step to $N$ independent computations, enabling parallelization and tractability.

### Responsibilities in Mixture Models

For mixture models with $K$ components, the E-step computes **responsibilities**:

$$
\gamma_{ik} = p(z_i = k | \mathbf{x}_i, \theta^{(t)}) = \frac{p(z_i = k | \theta^{(t)}) \, p(\mathbf{x}_i | z_i = k, \theta^{(t)})}{\sum_{j=1}^{K} p(z_i = j | \theta^{(t)}) \, p(\mathbf{x}_i | z_i = j, \theta^{(t)})}
$$

These responsibilities represent the **soft assignment** of each observation to each component and satisfy:

- $0 \leq \gamma_{ik} \leq 1$ for all $i, k$
- $\sum_{k=1}^{K} \gamma_{ik} = 1$ for each observation $i$

### E-Step Complexity

The computational cost of the E-step depends on the latent variable structure:

| Model | Latent Structure | E-Step Complexity |
|-------|-----------------|-------------------|
| Gaussian Mixture | Discrete, independent | $O(NK)$ |
| Hidden Markov Model | Discrete, sequential | $O(NK^2T)$ via forward-backward |
| Factor Analysis | Continuous Gaussian | $O(Nd^3)$ where $d$ is latent dimension |
| Latent Dirichlet Allocation | Discrete, coupled | Approximate inference required |

---

## E-Step Derivations for Common Models

### Gaussian Mixture Model

**Model**: $K$ Gaussian components with mixing proportions $\boldsymbol{\pi} = (\pi_1, \ldots, \pi_K)$, means $\{\boldsymbol{\mu}_k\}$, and covariances $\{\boldsymbol{\Sigma}_k\}$.

**Latent variable**: $z_i \in \{1, \ldots, K\}$ indicates component membership.

**E-step derivation**:

$$
\gamma_{ik} = \frac{\pi_k \, \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \, \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
$$

where the Gaussian density is:

$$
\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right)
$$

**Expected sufficient statistics**:

$$
N_k = \sum_{i=1}^{N} \gamma_{ik}, \quad \bar{\mathbf{x}}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i, \quad \bar{S}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \bar{\mathbf{x}}_k)(\mathbf{x}_i - \bar{\mathbf{x}}_k)^\top
$$

### Factor Analysis

**Model**: Observations generated by linear transformation of latent factors:

$$
\mathbf{x}_i = \mathbf{W} \mathbf{z}_i + \boldsymbol{\mu} + \boldsymbol{\epsilon}_i, \quad \mathbf{z}_i \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \quad \boldsymbol{\epsilon}_i \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Psi})
$$

where $\mathbf{W}$ is the factor loading matrix and $\boldsymbol{\Psi}$ is diagonal.

**E-step derivation**: The posterior over latents is Gaussian:

$$
p(\mathbf{z}_i | \mathbf{x}_i, \theta) = \mathcal{N}(\mathbf{z}_i | \mathbf{m}_i, \mathbf{V})
$$

with:

$$
\mathbf{V} = (\mathbf{I} + \mathbf{W}^\top \boldsymbol{\Psi}^{-1} \mathbf{W})^{-1}
$$

$$
\mathbf{m}_i = \mathbf{V} \mathbf{W}^\top \boldsymbol{\Psi}^{-1} (\mathbf{x}_i - \boldsymbol{\mu})
$$

**Expected sufficient statistics**:

$$
\mathbb{E}[\mathbf{z}_i | \mathbf{x}_i] = \mathbf{m}_i
$$

$$
\mathbb{E}[\mathbf{z}_i \mathbf{z}_i^\top | \mathbf{x}_i] = \mathbf{V} + \mathbf{m}_i \mathbf{m}_i^\top
$$

### Hidden Markov Model

**Model**: Sequence of latent states $\{z_1, \ldots, z_T\}$ with transition matrix $\mathbf{A}$ and emission distributions $\{p(\mathbf{x}_t | z_t)\}$.

**E-step via Forward-Backward Algorithm**:

**Forward pass** computes $\alpha_t(k) = p(\mathbf{x}_{1:t}, z_t = k)$:

$$
\alpha_1(k) = \pi_k \, p(\mathbf{x}_1 | z_1 = k)
$$

$$
\alpha_t(k) = p(\mathbf{x}_t | z_t = k) \sum_{j=1}^{K} \alpha_{t-1}(j) \, A_{jk}
$$

**Backward pass** computes $\beta_t(k) = p(\mathbf{x}_{t+1:T} | z_t = k)$:

$$
\beta_T(k) = 1
$$

$$
\beta_t(k) = \sum_{j=1}^{K} A_{kj} \, p(\mathbf{x}_{t+1} | z_{t+1} = j) \, \beta_{t+1}(j)
$$

**Expected sufficient statistics**:

$$
\gamma_t(k) = p(z_t = k | \mathbf{x}_{1:T}) = \frac{\alpha_t(k) \beta_t(k)}{\sum_{j} \alpha_t(j) \beta_t(j)}
$$

$$
\xi_t(j, k) = p(z_t = j, z_{t+1} = k | \mathbf{x}_{1:T}) = \frac{\alpha_t(j) A_{jk} p(\mathbf{x}_{t+1} | z_{t+1} = k) \beta_{t+1}(k)}{\sum_{j',k'} \alpha_t(j') A_{j'k'} p(\mathbf{x}_{t+1} | z_{t+1} = k') \beta_{t+1}(k')}
$$

---

## M-Step: Parameter Update Derivations

### The Optimization Problem

The M-step maximizes the **Q-function** (expected complete-data log-likelihood):

$$
\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})
$$

where:

$$
Q(\theta | \theta^{(t)}) = \mathbb{E}_{p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})}[\log p(\mathbf{X}, \mathbf{Z} | \theta)]
$$

### Why the Q-Function is Tractable

The Q-function is easier to optimize than the marginal log-likelihood because:

1. **Log inside expectation**: $\mathbb{E}[\log p(\mathbf{X}, \mathbf{Z} | \theta)]$ rather than $\log \mathbb{E}[p(\mathbf{X}, \mathbf{Z} | \theta)]$
2. **Exponential family structure**: For exponential families, Q is concave in natural parameters
3. **Parameter decoupling**: Different parameter groups often decouple in Q

### Deriving M-Step Updates

The general procedure:

1. Write out $\log p(\mathbf{X}, \mathbf{Z} | \theta)$ explicitly
2. Take expectation with respect to $p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$
3. Differentiate with respect to each parameter
4. Set gradient to zero and solve

### Complete Data Log-Likelihood Decomposition

For independent observations with factorized likelihood:

$$
\log p(\mathbf{X}, \mathbf{Z} | \theta) = \sum_{i=1}^{N} \log p(\mathbf{x}_i, z_i | \theta) = \sum_{i=1}^{N} \left[ \log p(z_i | \theta) + \log p(\mathbf{x}_i | z_i, \theta) \right]
$$

The Q-function then separates into prior and likelihood terms:

$$
Q(\theta | \theta^{(t)}) = \underbrace{\sum_{i=1}^{N} \mathbb{E}[\log p(z_i | \theta)]}_{\text{prior term}} + \underbrace{\sum_{i=1}^{N} \mathbb{E}[\log p(\mathbf{x}_i | z_i, \theta)]}_{\text{likelihood term}}
$$

---

## M-Step Derivations for Common Models

### Gaussian Mixture Model

**Q-function**:

$$
Q(\theta | \theta^{(t)}) = \sum_{i=1}^{N} \sum_{k=1}^{K} \gamma_{ik} \left[ \log \pi_k + \log \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
$$

Expanding the Gaussian log-density:

$$
Q = \sum_{i,k} \gamma_{ik} \log \pi_k - \frac{1}{2} \sum_{i,k} \gamma_{ik} \left[ d \log(2\pi) + \log|\boldsymbol{\Sigma}_k| + (\mathbf{x}_i - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) \right]
$$

**Mixing proportions** (with constraint $\sum_k \pi_k = 1$):

Using Lagrange multipliers:

$$
\frac{\partial}{\partial \pi_k} \left[ Q + \lambda \left( \sum_j \pi_j - 1 \right) \right] = \frac{N_k}{\pi_k} + \lambda = 0
$$

Solving: $\pi_k = -N_k / \lambda$. The constraint gives $\lambda = -N$, thus:

$$
\boxed{\pi_k^{(t+1)} = \frac{N_k}{N} = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}}
$$

**Means**:

$$
\frac{\partial Q}{\partial \boldsymbol{\mu}_k} = \sum_{i=1}^{N} \gamma_{ik} \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) = 0
$$

Solving:

$$
\boxed{\boldsymbol{\mu}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i}{\sum_{i=1}^{N} \gamma_{ik}} = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i}
$$

**Covariances**:

Using matrix calculus ($\frac{\partial}{\partial \boldsymbol{\Sigma}^{-1}} \log|\boldsymbol{\Sigma}^{-1}| = \boldsymbol{\Sigma}$):

$$
\frac{\partial Q}{\partial \boldsymbol{\Sigma}_k^{-1}} = \frac{N_k}{2} \boldsymbol{\Sigma}_k - \frac{1}{2} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k)(\mathbf{x}_i - \boldsymbol{\mu}_k)^\top = 0
$$

Solving:

$$
\boxed{\boldsymbol{\Sigma}_k^{(t+1)} = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k^{(t+1)})(\mathbf{x}_i - \boldsymbol{\mu}_k^{(t+1)})^\top}
$$

### Factor Analysis

**Q-function** (dropping constants):

$$
Q = -\frac{N}{2} \log|\boldsymbol{\Psi}| - \frac{1}{2} \sum_{i=1}^{N} \text{tr}\left( \boldsymbol{\Psi}^{-1} \mathbb{E}\left[ (\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{W}\mathbf{z}_i)(\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{W}\mathbf{z}_i)^\top \right] \right)
$$

**Loading matrix** $\mathbf{W}$:

$$
\frac{\partial Q}{\partial \mathbf{W}} = \boldsymbol{\Psi}^{-1} \sum_{i=1}^{N} \left[ (\mathbf{x}_i - \boldsymbol{\mu}) \mathbb{E}[\mathbf{z}_i]^\top - \mathbf{W} \mathbb{E}[\mathbf{z}_i \mathbf{z}_i^\top] \right] = 0
$$

Solving:

$$
\boxed{\mathbf{W}^{(t+1)} = \left( \sum_{i=1}^{N} (\mathbf{x}_i - \boldsymbol{\mu}) \mathbf{m}_i^\top \right) \left( \sum_{i=1}^{N} (\mathbf{V} + \mathbf{m}_i \mathbf{m}_i^\top) \right)^{-1}}
$$

**Noise variance** $\boldsymbol{\Psi}$ (diagonal elements):

$$
\boxed{\Psi_{jj}^{(t+1)} = \frac{1}{N} \sum_{i=1}^{N} \left[ (x_{ij} - \mu_j)^2 - 2 W_j^{(t+1)} m_i (x_{ij} - \mu_j) + W_j^{(t+1)} (\mathbf{V} + \mathbf{m}_i \mathbf{m}_i^\top) W_j^{(t+1)\top} \right]}
$$

where $W_j$ is the $j$-th row of $\mathbf{W}$.

### Hidden Markov Model

**Q-function**:

$$
Q = \sum_{k=1}^{K} \gamma_1(k) \log \pi_k + \sum_{t=1}^{T-1} \sum_{j,k} \xi_t(j,k) \log A_{jk} + \sum_{t=1}^{T} \sum_{k=1}^{K} \gamma_t(k) \log p(\mathbf{x}_t | z_t = k, \theta)
$$

**Initial distribution**:

$$
\boxed{\pi_k^{(t+1)} = \gamma_1(k)}
$$

**Transition matrix**:

$$
\boxed{A_{jk}^{(t+1)} = \frac{\sum_{t=1}^{T-1} \xi_t(j,k)}{\sum_{t=1}^{T-1} \gamma_t(j)}}
$$

**Emission parameters**: Depend on emission distribution family (Gaussian, multinomial, etc.), updated using responsibilities $\gamma_t(k)$ as weights.

---

## Numerical Stability Considerations

### Log-Space Computation

Direct computation of responsibilities can underflow:

$$
\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
$$

**Solution**: Compute in log-space using the log-sum-exp trick:

$$
\log \gamma_{ik} = \log \pi_k + \log \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) - \text{LogSumExp}_j \left( \log \pi_j + \log \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j) \right)
$$

where:

$$
\text{LogSumExp}(a_1, \ldots, a_K) = a_{\max} + \log \sum_{k=1}^{K} \exp(a_k - a_{\max})
$$

### Covariance Regularization

Singular covariances cause numerical issues. Common fixes:

1. **Diagonal loading**: $\boldsymbol{\Sigma}_k \leftarrow \boldsymbol{\Sigma}_k + \epsilon \mathbf{I}$
2. **Minimum eigenvalue**: Clamp eigenvalues above threshold
3. **Tied covariances**: Share $\boldsymbol{\Sigma}$ across components
4. **Diagonal covariances**: Restrict to $\boldsymbol{\Sigma}_k = \text{diag}(\sigma_{k1}^2, \ldots, \sigma_{kd}^2)$

### Component Collapse Prevention

When $N_k \to 0$ (empty component):

1. **Reinitialize**: Reset component to random data point
2. **Merge and split**: Merge empty with largest, split largest
3. **Regularization**: Add prior counts (Bayesian approach)

---

## PyTorch Implementation

```python
import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal, Categorical

class GaussianMixtureEM:
    """Gaussian Mixture Model with EM algorithm."""
    
    def __init__(self, n_components: int, n_features: int, 
                 covariance_type: str = 'full', reg_covar: float = 1e-6):
        self.K = n_components
        self.d = n_features
        self.covariance_type = covariance_type
        self.reg_covar = reg_covar
        
        # Initialize parameters
        self.pi = torch.ones(self.K) / self.K  # Mixing proportions
        self.mu = torch.randn(self.K, self.d)   # Means
        self.Sigma = torch.stack([torch.eye(self.d) for _ in range(self.K)])  # Covariances
        
    def e_step(self, X: torch.Tensor) -> torch.Tensor:
        """
        E-step: Compute responsibilities.
        
        Args:
            X: Data tensor of shape (N, d)
            
        Returns:
            gamma: Responsibilities of shape (N, K)
        """
        N = X.shape[0]
        log_resp = torch.zeros(N, self.K)
        
        for k in range(self.K):
            mvn = MultivariateNormal(self.mu[k], self.Sigma[k])
            log_resp[:, k] = torch.log(self.pi[k]) + mvn.log_prob(X)
        
        # Log-sum-exp for numerical stability
        log_resp_sum = torch.logsumexp(log_resp, dim=1, keepdim=True)
        log_gamma = log_resp - log_resp_sum
        gamma = torch.exp(log_gamma)
        
        return gamma
    
    def m_step(self, X: torch.Tensor, gamma: torch.Tensor):
        """
        M-step: Update parameters.
        
        Args:
            X: Data tensor of shape (N, d)
            gamma: Responsibilities of shape (N, K)
        """
        N = X.shape[0]
        
        # Effective counts
        N_k = gamma.sum(dim=0)  # (K,)
        
        # Update mixing proportions
        self.pi = N_k / N
        
        # Update means
        self.mu = (gamma.T @ X) / N_k.unsqueeze(1)  # (K, d)
        
        # Update covariances
        for k in range(self.K):
            diff = X - self.mu[k]  # (N, d)
            weighted_diff = gamma[:, k].unsqueeze(1) * diff  # (N, d)
            self.Sigma[k] = (weighted_diff.T @ diff) / N_k[k]
            
            # Regularization
            self.Sigma[k] += self.reg_covar * torch.eye(self.d)
    
    def log_likelihood(self, X: torch.Tensor) -> float:
        """Compute log-likelihood of data."""
        N = X.shape[0]
        log_prob = torch.zeros(N, self.K)
        
        for k in range(self.K):
            mvn = MultivariateNormal(self.mu[k], self.Sigma[k])
            log_prob[:, k] = torch.log(self.pi[k]) + mvn.log_prob(X)
        
        return torch.logsumexp(log_prob, dim=1).sum().item()
    
    def fit(self, X: torch.Tensor, max_iters: int = 100, 
            tol: float = 1e-4, verbose: bool = False):
        """
        Fit GMM using EM algorithm.
        
        Args:
            X: Data tensor of shape (N, d)
            max_iters: Maximum number of iterations
            tol: Convergence tolerance
            verbose: Print progress
        """
        prev_ll = float('-inf')
        
        for iteration in range(max_iters):
            # E-step
            gamma = self.e_step(X)
            
            # M-step
            self.m_step(X, gamma)
            
            # Check convergence
            ll = self.log_likelihood(X)
            
            if verbose and iteration % 10 == 0:
                print(f"Iteration {iteration}: log-likelihood = {ll:.4f}")
            
            if abs(ll - prev_ll) < tol:
                if verbose:
                    print(f"Converged at iteration {iteration}")
                break
            
            prev_ll = ll
        
        return self
    
    def predict(self, X: torch.Tensor) -> torch.Tensor:
        """Predict cluster assignments."""
        gamma = self.e_step(X)
        return gamma.argmax(dim=1)
    
    def predict_proba(self, X: torch.Tensor) -> torch.Tensor:
        """Predict cluster probabilities (responsibilities)."""
        return self.e_step(X)


# Example usage
if __name__ == "__main__":
    torch.manual_seed(42)
    
    # Generate synthetic data from 3-component GMM
    N = 300
    true_means = torch.tensor([[0., 0.], [3., 3.], [-2., 3.]])
    
    X = torch.cat([
        torch.randn(100, 2) + true_means[0],
        torch.randn(100, 2) + true_means[1],
        torch.randn(100, 2) + true_means[2]
    ])
    
    # Fit GMM
    gmm = GaussianMixtureEM(n_components=3, n_features=2)
    gmm.fit(X, verbose=True)
    
    print(f"\nLearned means:\n{gmm.mu}")
    print(f"\nLearned mixing proportions: {gmm.pi}")
```

---

## Summary

| Step | Input | Output | Computation |
|------|-------|--------|-------------|
| **E-step** | Current $\theta^{(t)}$, data $\mathbf{X}$ | Responsibilities $\gamma$ or posterior moments | Bayes' theorem, forward-backward |
| **M-step** | Responsibilities $\gamma$, data $\mathbf{X}$ | Updated $\theta^{(t+1)}$ | Weighted MLE, closed-form or optimization |

### Key Insights

1. **E-step simplification**: Often only expected sufficient statistics are needed, not the full posterior
2. **M-step tractability**: Exponential family structure yields closed-form updates
3. **Numerical stability**: Log-space computation and regularization are essential in practice
4. **Independence**: Factorization over data points enables efficient, parallelizable implementation

The alternation between these steps—computing what we expect the latent variables to be (E) and then optimizing parameters as if those expectations were true (M)—is the elegant core of the EM algorithm.

---

# Appendix: The EM Iteration — Detailed Derivation

# The EM Iteration

The EM algorithm alternates between two steps: the E-step (Expectation) and the M-step (Maximization). This section provides complete derivations of both steps, proves the monotonic improvement guarantee, and interprets EM as coordinate ascent on the ELBO.

---

## E-Step (Expectation)

### Computing the Posterior over Latents

The E-step computes the **posterior distribution** over latent variables given the current parameter estimate $\theta^{(t)}$:

$$
q^{(t+1)}(\mathbf{Z}) = p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})
$$

Using Bayes' theorem:

$$
p(\mathbf{Z} | \mathbf{X}, \theta^{(t)}) = \frac{p(\mathbf{X}, \mathbf{Z} | \theta^{(t)})}{p(\mathbf{X} | \theta^{(t)})} = \frac{p(\mathbf{X} | \mathbf{Z}, \theta^{(t)}) \, p(\mathbf{Z} | \theta^{(t)})}{\int p(\mathbf{X}, \mathbf{Z}' | \theta^{(t)}) \, d\mathbf{Z}'}
$$

**Key Point**: The E-step requires computing the posterior, which involves the same integral that made direct optimization intractable. However, for many models (especially those in the exponential family), this posterior has a tractable closed form.

### Example: Gaussian Mixture Model

For a GMM with $K$ components, the latent variable $z_i \in \{1, \ldots, K\}$ indicates cluster membership for observation $\mathbf{x}_i$. The E-step computes the **responsibilities**:

$$
\gamma_{ik} = p(z_i = k | \mathbf{x}_i, \theta^{(t)}) = \frac{\pi_k^{(t)} \, \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k^{(t)}, \boldsymbol{\Sigma}_k^{(t)})}{\sum_{j=1}^{K} \pi_j^{(t)} \, \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j^{(t)}, \boldsymbol{\Sigma}_j^{(t)})}
$$

These responsibilities represent the soft assignment of each data point to each cluster.

### Making the Bound Tight

The E-step serves a crucial purpose: it makes the ELBO **tight** at the current parameter value $\theta^{(t)}$.

Recall the fundamental decomposition:

$$
\ell(\theta) = \mathcal{L}(q, \theta) + D_{\mathrm{KL}}\bigl(q(\mathbf{Z}) \,\|\, p(\mathbf{Z} | \mathbf{X}, \theta)\bigr)
$$

When we set $q(\mathbf{Z}) = p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$:

$$
D_{\mathrm{KL}}\bigl(p(\mathbf{Z} | \mathbf{X}, \theta^{(t)}) \,\|\, p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})\bigr) = 0
$$

Therefore:

$$
\mathcal{L}(q^{(t+1)}, \theta^{(t)}) = \ell(\theta^{(t)})
$$

**The ELBO equals the log-likelihood at $\theta^{(t)}$**—the bound is tight.

### Why Tightness Matters

Making the bound tight at the current point ensures that:

1. Any improvement in the ELBO translates to improvement in the log-likelihood
2. The algorithm doesn't get stuck at suboptimal points due to a loose bound
3. We have a precise starting point for the M-step optimization

---

## M-Step (Maximization)

### The Q-Function

The M-step maximizes the ELBO with respect to $\theta$, holding $q$ fixed at $q^{(t+1)}$. Since $q^{(t+1)} = p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$, the ELBO becomes:

$$
\mathcal{L}(q^{(t+1)}, \theta) = \mathbb{E}_{p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})}[\log p(\mathbf{X}, \mathbf{Z} | \theta)] + H[q^{(t+1)}]
$$

The entropy term $H[q^{(t+1)}]$ does not depend on $\theta$, so maximizing $\mathcal{L}$ is equivalent to maximizing the **Q-function**:

$$
Q(\theta | \theta^{(t)}) = \mathbb{E}_{p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})}[\log p(\mathbf{X}, \mathbf{Z} | \theta)]
$$

This is the **expected complete-data log-likelihood**, where the expectation is taken over the posterior distribution of latent variables computed in the E-step.

### Q-Function Optimization

The M-step finds:

$$
\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})
$$

**Why Q is Easier to Optimize**:

1. **No integral over $\theta$**: The log is inside the expectation, not outside an integral
2. **Exponential family structure**: For exponential family models, $Q$ often has closed-form maximizers
3. **Decoupling**: Parameters often decouple, allowing separate optimization

### Expected Complete-Data Log-Likelihood

Expanding the Q-function:

$$
Q(\theta | \theta^{(t)}) = \int p(\mathbf{Z} | \mathbf{X}, \theta^{(t)}) \log p(\mathbf{X}, \mathbf{Z} | \theta) \, d\mathbf{Z}
$$

For discrete latent variables:

$$
Q(\theta | \theta^{(t)}) = \sum_{\mathbf{Z}} p(\mathbf{Z} | \mathbf{X}, \theta^{(t)}) \log p(\mathbf{X}, \mathbf{Z} | \theta)
$$

### Example: GMM M-Step

For a Gaussian Mixture Model, the M-step updates are:

**Mixing proportions**:

$$
\pi_k^{(t+1)} = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}
$$

**Means**:

$$
\boldsymbol{\mu}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma_{ik} \, \mathbf{x}_i}{\sum_{i=1}^{N} \gamma_{ik}}
$$

**Covariances**:

$$
\boldsymbol{\Sigma}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma_{ik} \, (\mathbf{x}_i - \boldsymbol{\mu}_k^{(t+1)})(\mathbf{x}_i - \boldsymbol{\mu}_k^{(t+1)})^\top}{\sum_{i=1}^{N} \gamma_{ik}}
$$

These are weighted versions of the standard maximum likelihood estimators, where the weights are the responsibilities from the E-step.

### Partial M-Step (Generalized EM)

In some cases, finding the global maximum of $Q(\theta | \theta^{(t)})$ is difficult. **Generalized EM (GEM)** only requires:

$$
Q(\theta^{(t+1)} | \theta^{(t)}) \geq Q(\theta^{(t)} | \theta^{(t)})
$$

Any improvement in $Q$ suffices—we don't need the global maximum. This is useful when:

- The M-step involves constrained optimization
- Closed-form solutions don't exist
- Gradient-based methods are used

---

## Monotonic Improvement Guarantee

### The Central Theorem

**Theorem (Monotonic Improvement)**: For any EM iteration, the log-likelihood never decreases:

$$
\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)})
$$

with equality if and only if $\theta^{(t+1)} = \theta^{(t)}$ (i.e., we are at a fixed point).

### Proof

We establish this through a chain of inequalities:

$$
\ell(\theta^{(t+1)}) \geq \mathcal{L}(q^{(t+1)}, \theta^{(t+1)}) \geq \mathcal{L}(q^{(t+1)}, \theta^{(t)}) = \ell(\theta^{(t)})
$$

**Step 1 — ELBO is a Lower Bound**:

$$
\ell(\theta^{(t+1)}) \geq \mathcal{L}(q^{(t+1)}, \theta^{(t+1)})
$$

This holds because the ELBO is **always** a lower bound on the log-likelihood for any $q$ and any $\theta$:

$$
\ell(\theta) = \mathcal{L}(q, \theta) + D_{\mathrm{KL}}(q \| p(\mathbf{Z}|\mathbf{X}, \theta)) \geq \mathcal{L}(q, \theta)
$$

since $D_{\mathrm{KL}} \geq 0$.

**Step 2 — M-Step Improves ELBO**:

$$
\mathcal{L}(q^{(t+1)}, \theta^{(t+1)}) \geq \mathcal{L}(q^{(t+1)}, \theta^{(t)})
$$

This holds by definition of the M-step: $\theta^{(t+1)}$ is chosen to **maximize** $\mathcal{L}(q^{(t+1)}, \theta)$ over $\theta$.

**Step 3 — E-Step Makes Bound Tight**:

$$
\mathcal{L}(q^{(t+1)}, \theta^{(t)}) = \ell(\theta^{(t)})
$$

This holds because the E-step sets $q^{(t+1)} = p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$, making the KL divergence zero:

$$
D_{\mathrm{KL}}(q^{(t+1)} \| p(\mathbf{Z}|\mathbf{X}, \theta^{(t)})) = 0
$$

**Combining all steps**:

$$
\ell(\theta^{(t+1)}) \geq \mathcal{L}(q^{(t+1)}, \theta^{(t+1)}) \geq \mathcal{L}(q^{(t+1)}, \theta^{(t)}) = \ell(\theta^{(t)})
$$

### Geometric Picture

```
Log-likelihood ℓ(θ)
         │
         │     ╱───────────────  ℓ(θ)
         │    ╱        ●──────────○  ℓ(θ^(t+1))
         │   ╱        ╱
         │  ╱    ●───●  ELBO at iteration t
         │ ╱    ╱   ↑
         │╱    ╱    M-step maximizes bound
         │    ● ────┘
         │   ↑ θ^(t)
         │   E-step makes bound tight here
         └────────────────────────────────────── θ
```

1. **E-step**: Construct a lower bound that touches $\ell(\theta)$ at $\theta^{(t)}$
2. **M-step**: Move to $\theta^{(t+1)}$ that maximizes the bound
3. At $\theta^{(t+1)}$, the bound may be loose, but $\ell(\theta^{(t+1)})$ is even higher

### Strict Improvement

If $\theta^{(t+1)} \neq \theta^{(t)}$ and the M-step achieves a strict improvement in the bound, then:

$$
\ell(\theta^{(t+1)}) > \ell(\theta^{(t)})
$$

This rules out cycles in the algorithm—EM either converges to a fixed point or strictly improves at each iteration.

### When Does Improvement Stop?

The sequence $\{\ell(\theta^{(t)})\}$ is monotonically increasing and bounded above (assuming the likelihood is proper). By the **monotone convergence theorem**, the sequence converges.

**Stationarity condition**: At convergence, $\theta^* = \theta^{(t)} = \theta^{(t+1)}$, meaning:

$$
\nabla_\theta Q(\theta | \theta^*)\big|_{\theta = \theta^*} = \nabla_\theta \mathcal{L}(q^*, \theta)\big|_{\theta = \theta^*} = 0
$$

This is a **necessary condition** for local optimality of $\ell(\theta)$, but not sufficient for global optimality—EM can converge to local maxima or saddle points.

---

## Coordinate Ascent Interpretation

### ELBO as Joint Objective

The EM algorithm can be understood as **coordinate ascent** on the functional $\mathcal{L}(q, \theta)$, which depends on both a distribution $q(\mathbf{Z})$ and parameters $\theta$.

### Two-Block Optimization

**E-step**: Maximize $\mathcal{L}(q, \theta)$ over $q$, holding $\theta = \theta^{(t)}$ fixed:

$$
q^{(t+1)} = \arg\max_q \mathcal{L}(q, \theta^{(t)})
$$

The solution is $q^{(t+1)} = p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$, which sets $D_{\mathrm{KL}} = 0$.

**M-step**: Maximize $\mathcal{L}(q, \theta)$ over $\theta$, holding $q = q^{(t+1)}$ fixed:

$$
\theta^{(t+1)} = \arg\max_\theta \mathcal{L}(q^{(t+1)}, \theta)
$$

### Why Coordinate Ascent Works

Each step increases (or maintains) the ELBO:

$$
\mathcal{L}(q^{(t)}, \theta^{(t)}) \leq \mathcal{L}(q^{(t+1)}, \theta^{(t)}) \leq \mathcal{L}(q^{(t+1)}, \theta^{(t+1)})
$$

Since the ELBO lower-bounds the log-likelihood, and the E-step makes the bound tight, improvements in the ELBO translate to improvements in $\ell(\theta)$.

### Functional Optimization in E-Step

The E-step is a **functional optimization** problem: we optimize over the space of all distributions $q(\mathbf{Z})$. This is an infinite-dimensional optimization!

**Remarkably**, the solution has a closed form. Using calculus of variations or Lagrange multipliers:

$$
\frac{\delta}{\delta q(\mathbf{Z})} \left[ \mathcal{L}(q, \theta) - \lambda \left( \int q(\mathbf{Z}) d\mathbf{Z} - 1 \right) \right] = 0
$$

This yields:

$$
\log q^*(\mathbf{Z}) = \log p(\mathbf{X}, \mathbf{Z} | \theta) - \log p(\mathbf{X} | \theta)
$$

Thus $q^*(\mathbf{Z}) = p(\mathbf{Z} | \mathbf{X}, \theta)$—the posterior distribution.

### Connection to Block Coordinate Descent

The coordinate ascent view connects EM to a broader class of optimization algorithms:

| Algorithm | Variables | Update Rule |
|-----------|-----------|-------------|
| EM | $(q, \theta)$ | Alternating maximization |
| Gibbs Sampling | $(z_1, \ldots, z_d)$ | Cycle through conditionals |
| ADMM | $(x, z, u)$ | Alternating with dual update |

All share the property of optimizing one block while holding others fixed.

### Implications of the Coordinate Ascent View

1. **Convergence**: Standard results for coordinate ascent apply—EM converges to a stationary point under mild regularity conditions

2. **Rate of Convergence**: The convergence rate depends on the curvature of $\mathcal{L}$ and the coupling between $q$ and $\theta$

3. **Generalizations**: This view motivates variants like:
   - **Partial E-step**: Don't fully optimize over $q$
   - **Partial M-step**: Don't fully optimize over $\theta$ (Generalized EM)
   - **Variational EM**: Restrict $q$ to a tractable family

4. **Connection to Variational Inference**: When the exact posterior is intractable, we can restrict $q$ to a variational family and still perform coordinate ascent—this is **variational inference**.

---

## Summary: The Complete EM Iteration

Given current parameters $\theta^{(t)}$:

### E-Step

1. Compute the posterior over latent variables:
   $$q^{(t+1)}(\mathbf{Z}) = p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$$

2. This makes the ELBO tight at $\theta^{(t)}$:
   $$\mathcal{L}(q^{(t+1)}, \theta^{(t)}) = \ell(\theta^{(t)})$$

### M-Step

1. Define the Q-function:
   $$Q(\theta | \theta^{(t)}) = \mathbb{E}_{q^{(t+1)}}[\log p(\mathbf{X}, \mathbf{Z} | \theta)]$$

2. Maximize to get new parameters:
   $$\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})$$

### Guarantees

- **Monotonic improvement**: $\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)})$
- **Convergence**: The sequence $\{\theta^{(t)}\}$ converges to a stationary point
- **No cycles**: Either strict improvement or convergence at each step

### Algorithm Summary

```
Initialize θ⁽⁰⁾
repeat until convergence:
    # E-step: compute posterior
    q(Z) ← p(Z | X, θ⁽ᵗ⁾)
    
    # M-step: maximize expected complete-data log-likelihood
    θ⁽ᵗ⁺¹⁾ ← argmax_θ E_q[log p(X, Z | θ)]
    
    t ← t + 1
return θ⁽ᵗ⁾
```
