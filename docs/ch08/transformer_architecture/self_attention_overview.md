# Step 2: Self-Attention Deep Dive
## Introduction
Master the core mechanism behind Transformers: self-attention.
## Files
- self_attention.py - Self-attention implementation
- scaled_dot_product.py - Scaled attention formula
- qkv_projection.py - Query-Key-Value projections
- demo.py - Interactive demonstrations
