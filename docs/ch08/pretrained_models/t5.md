# T5: Text-to-Text Transfer Transformer

## Introduction

T5 (Text-to-Text Transfer Transformer) unifies all NLP tasks into a single text-to-text format. Instead of task-specific architectures, every task is framed as generating output text from input text, making the model remarkably versatile.

## The Text-to-Text Framework

T5's key insight is that all NLP tasks can be expressed as text generation:

| Task | Input | Output |
|------|-------|--------|
| Translation | `translate English to German: That is good.` | `Das ist gut.` |
| Summarization | `summarize: <article>` | `<summary>` |
| Classification | `sentiment: This movie is great!` | `positive` |
| QA | `question: Who wrote Harry Potter? context: ...` | `J.K. Rowling` |

This unified format enables:
- Single model for all tasks
- Transfer learning across tasks
- Simple fine-tuning procedure

## Architecture

T5 uses the original encoder-decoder Transformer architecture:

$$
\text{T5} = \text{Encoder} + \text{Decoder}
$$

### Key Design Choices

| Aspect | T5 Choice | Alternative |
|--------|-----------|-------------|
| Architecture | Encoder-Decoder | Decoder-only (GPT) |
| Attention | Relative position bias | Absolute positional encoding |
| Normalization | Pre-norm | Post-norm |
| Activation | GELU (later ReLU variants) | ReLU |
| Embedding | Shared across encoder/decoder | Separate |

### Model Sizes

| Model | Encoder/Decoder Layers | $d_{\text{model}}$ | Parameters |
|-------|------------------------|---------|------------|
| T5-Small | 6/6 | 512 | 60M |
| T5-Base | 12/12 | 768 | 220M |
| T5-Large | 24/24 | 1024 | 770M |
| T5-3B | 24/24 | 1024 | 3B |
| T5-11B | 24/24 | 1024 | 11B |

## Pre-training: Span Corruption

T5 uses a denoising objective called **span corruption**:

1. Randomly select 15% of tokens
2. Replace consecutive spans with sentinel tokens `<extra_id_0>`, `<extra_id_1>`, etc.
3. Train the model to reconstruct the original spans

**Example:**

```
Original: "Thank you for inviting me to your party last week."
Input: "Thank you <extra_id_0> to your party <extra_id_1> week."
Target: "<extra_id_0> for inviting me <extra_id_1> last"
```

This is more efficient than BERT's token-level masking because multiple tokens are predicted per sentinel.

### Comparison of Pre-training Objectives

| Objective | Model | Tokens Predicted | Bidirectional? |
|-----------|-------|-----------------|----------------|
| Masked LM (MLM) | BERT | ~15% of input tokens | Yes |
| Causal LM (CLM) | GPT | 100% of tokens (each predicts next) | No |
| Span Corruption | T5 | ~15% of tokens (via spans) | Encoder: Yes, Decoder: No |

T5's span corruption balances between BERT's bidirectional understanding and GPT's generative capability. The encoder processes the corrupted input with full bidirectional context, while the decoder generates missing spans autoregressively.

### Multi-task Training

T5 demonstrated that training on a mixture of tasks during pre-training improves downstream performance. The C4 (Colossal Clean Crawled Corpus) dataset provides the unsupervised pre-training signal, while supervised tasks can be mixed in proportionally:

$$\mathcal{L} = \sum_{\text{task}} w_{\text{task}} \cdot \mathcal{L}_{\text{task}}$$

Flan-T5 extends this by instruction-tuning on over 1,800 tasks, showing that diverse multi-task training dramatically improves zero-shot generalization.

## PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, Tuple, List


class T5Config:
    """T5 model configuration."""
    
    def __init__(
        self,
        vocab_size: int = 32128,
        d_model: int = 512,
        d_kv: int = 64,
        d_ff: int = 2048,
        num_layers: int = 6,
        num_decoder_layers: int = None,
        num_heads: int = 8,
        relative_attention_num_buckets: int = 32,
        relative_attention_max_distance: int = 128,
        dropout_rate: float = 0.1,
        layer_norm_epsilon: float = 1e-6,
        feed_forward_proj: str = "relu"
    ):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.d_kv = d_kv
        self.d_ff = d_ff
        self.num_layers = num_layers
        self.num_decoder_layers = num_decoder_layers or num_layers
        self.num_heads = num_heads
        self.relative_attention_num_buckets = relative_attention_num_buckets
        self.relative_attention_max_distance = relative_attention_max_distance
        self.dropout_rate = dropout_rate
        self.layer_norm_epsilon = layer_norm_epsilon
        self.feed_forward_proj = feed_forward_proj


class T5RelativePositionBias(nn.Module):
    """
    T5-style relative position bias.
    
    Uses bucketed relative positions for efficiency.
    """
    
    def __init__(
        self,
        num_heads: int,
        num_buckets: int = 32,
        max_distance: int = 128,
        is_decoder: bool = False
    ):
        super().__init__()
        
        self.num_heads = num_heads
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        self.is_decoder = is_decoder
        
        self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)
    
    @staticmethod
    def _relative_position_bucket(
        relative_position: torch.Tensor,
        bidirectional: bool = True,
        num_buckets: int = 32,
        max_distance: int = 128
    ) -> torch.Tensor:
        """
        Compute relative position buckets.
        
        Buckets are logarithmically spaced for distant positions.
        """
        ret = 0
        n = -relative_position
        
        if bidirectional:
            num_buckets //= 2
            ret += (n < 0).to(torch.long) * num_buckets
            n = torch.abs(n)
        else:
            n = torch.max(n, torch.zeros_like(n))
        
        # Half buckets are for exact positions
        max_exact = num_buckets // 2
        is_small = n < max_exact
        
        # Other half are for logarithmically bigger bins
        val_if_large = max_exact + (
            torch.log(n.float() / max_exact) /
            math.log(max_distance / max_exact) *
            (num_buckets - max_exact)
        ).to(torch.long)
        
        val_if_large = torch.min(
            val_if_large,
            torch.full_like(val_if_large, num_buckets - 1)
        )
        
        ret += torch.where(is_small, n, val_if_large)
        return ret
    
    def forward(self, query_length: int, key_length: int) -> torch.Tensor:
        """
        Compute relative position bias.
        
        Returns: [1, num_heads, query_length, key_length]
        """
        device = self.relative_attention_bias.weight.device
        
        # Relative positions
        context_position = torch.arange(query_length, device=device)[:, None]
        memory_position = torch.arange(key_length, device=device)[None, :]
        relative_position = memory_position - context_position
        
        # Bucket the positions
        relative_position_bucket = self._relative_position_bucket(
            relative_position,
            bidirectional=not self.is_decoder,
            num_buckets=self.num_buckets,
            max_distance=self.max_distance
        )
        
        # Get bias values
        values = self.relative_attention_bias(relative_position_bucket)
        values = values.permute([2, 0, 1]).unsqueeze(0)
        
        return values


class T5Attention(nn.Module):
    """T5 Multi-Head Attention with relative position bias."""
    
    def __init__(
        self,
        config: T5Config,
        is_decoder: bool = False,
        has_relative_attention_bias: bool = False
    ):
        super().__init__()
        
        self.is_decoder = is_decoder
        self.has_relative_attention_bias = has_relative_attention_bias
        
        self.d_model = config.d_model
        self.d_kv = config.d_kv
        self.num_heads = config.num_heads
        self.inner_dim = self.num_heads * self.d_kv
        
        # Projections
        self.q = nn.Linear(config.d_model, self.inner_dim, bias=False)
        self.k = nn.Linear(config.d_model, self.inner_dim, bias=False)
        self.v = nn.Linear(config.d_model, self.inner_dim, bias=False)
        self.o = nn.Linear(self.inner_dim, config.d_model, bias=False)
        
        # Relative position bias
        if has_relative_attention_bias:
            self.relative_attention_bias = T5RelativePositionBias(
                num_heads=config.num_heads,
                num_buckets=config.relative_attention_num_buckets,
                max_distance=config.relative_attention_max_distance,
                is_decoder=is_decoder
            )
        
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = None,
        position_bias: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """
        Forward pass.
        
        Args:
            hidden_states: [batch, seq_len, d_model]
            key_value_states: For cross-attention, encoder hidden states
            position_bias: Pre-computed position bias
            attention_mask: Attention mask
            past_key_value: Cached KV for incremental decoding
            use_cache: Whether to return updated cache
        """
        batch_size, seq_len = hidden_states.shape[:2]
        
        # Self-attention or cross-attention
        is_cross_attention = key_value_states is not None
        
        # Compute Q
        query_states = self.q(hidden_states)
        query_states = query_states.view(batch_size, -1, self.num_heads, self.d_kv).transpose(1, 2)
        
        # Compute K, V (from encoder for cross-attention)
        if is_cross_attention:
            key_states = self.k(key_value_states)
            value_states = self.v(key_value_states)
        else:
            key_states = self.k(hidden_states)
            value_states = self.v(hidden_states)
        
        key_states = key_states.view(batch_size, -1, self.num_heads, self.d_kv).transpose(1, 2)
        value_states = value_states.view(batch_size, -1, self.num_heads, self.d_kv).transpose(1, 2)
        
        # Handle caching
        if past_key_value is not None:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        
        present_key_value = (key_states, value_states) if use_cache else None
        
        # Attention scores
        scores = torch.matmul(query_states, key_states.transpose(-2, -1))
        
        # Add position bias
        if position_bias is None and self.has_relative_attention_bias:
            position_bias = self.relative_attention_bias(
                query_length=seq_len,
                key_length=key_states.size(2)
            )
        
        if position_bias is not None:
            scores += position_bias
        
        # Apply attention mask
        if attention_mask is not None:
            scores += attention_mask
        
        # Softmax and dropout
        attn_weights = F.softmax(scores.float(), dim=-1).type_as(scores)
        attn_weights = self.dropout(attn_weights)
        
        # Apply to values
        attn_output = torch.matmul(attn_weights, value_states)
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, -1, self.inner_dim)
        attn_output = self.o(attn_output)
        
        return attn_output, position_bias, present_key_value


class T5LayerFF(nn.Module):
    """T5 Feed-Forward Layer."""
    
    def __init__(self, config: T5Config):
        super().__init__()
        
        self.DenseReluDense = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff, bias=False),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.d_ff, config.d_model, bias=False)
        )
        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """Pre-norm FFN."""
        normed = self.layer_norm(hidden_states)
        ff_output = self.DenseReluDense(normed)
        return hidden_states + self.dropout(ff_output)


class T5EncoderBlock(nn.Module):
    """T5 Encoder Block."""
    
    def __init__(self, config: T5Config, has_relative_attention_bias: bool = False):
        super().__init__()
        
        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.self_attention = T5Attention(
            config,
            is_decoder=False,
            has_relative_attention_bias=has_relative_attention_bias
        )
        self.feed_forward = T5LayerFF(config)
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_bias: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Forward pass."""
        # Self-attention
        normed = self.layer_norm(hidden_states)
        attn_output, position_bias, _ = self.self_attention(
            normed,
            position_bias=position_bias,
            attention_mask=attention_mask
        )
        hidden_states = hidden_states + self.dropout(attn_output)
        
        # FFN
        hidden_states = self.feed_forward(hidden_states)
        
        return hidden_states, position_bias


class T5DecoderBlock(nn.Module):
    """T5 Decoder Block with self-attention and cross-attention."""
    
    def __init__(self, config: T5Config, has_relative_attention_bias: bool = False):
        super().__init__()
        
        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        
        # Masked self-attention
        self.self_attention = T5Attention(
            config,
            is_decoder=True,
            has_relative_attention_bias=has_relative_attention_bias
        )
        
        # Cross-attention
        self.cross_attention_layer_norm = nn.LayerNorm(
            config.d_model, eps=config.layer_norm_epsilon
        )
        self.cross_attention = T5Attention(config, is_decoder=True)
        
        # FFN
        self.feed_forward = T5LayerFF(config)
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        position_bias: Optional[torch.Tensor] = None,
        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple] = None,
        use_cache: bool = False
    ) -> Tuple:
        """Forward pass."""
        self_attn_past_key_value = past_key_value[:2] if past_key_value else None
        cross_attn_past_key_value = past_key_value[2:] if past_key_value else None
        
        # Self-attention
        normed = self.layer_norm(hidden_states)
        attn_output, position_bias, present_key_value = self.self_attention(
            normed,
            position_bias=position_bias,
            attention_mask=attention_mask,
            past_key_value=self_attn_past_key_value,
            use_cache=use_cache
        )
        hidden_states = hidden_states + self.dropout(attn_output)
        
        # Cross-attention
        normed = self.cross_attention_layer_norm(hidden_states)
        cross_attn_output, _, cross_present = self.cross_attention(
            normed,
            key_value_states=encoder_hidden_states,
            attention_mask=encoder_attention_mask,
            past_key_value=cross_attn_past_key_value,
            use_cache=use_cache
        )
        hidden_states = hidden_states + self.dropout(cross_attn_output)
        
        # FFN
        hidden_states = self.feed_forward(hidden_states)
        
        present = present_key_value + cross_present if use_cache else None
        
        return hidden_states, position_bias, present


class T5Model(nn.Module):
    """Complete T5 Encoder-Decoder Model."""
    
    def __init__(self, config: T5Config):
        super().__init__()
        self.config = config
        
        # Shared embeddings
        self.shared = nn.Embedding(config.vocab_size, config.d_model)
        
        # Encoder
        self.encoder_embed_dropout = nn.Dropout(config.dropout_rate)
        self.encoder_blocks = nn.ModuleList([
            T5EncoderBlock(config, has_relative_attention_bias=(i == 0))
            for i in range(config.num_layers)
        ])
        self.encoder_final_layer_norm = nn.LayerNorm(
            config.d_model, eps=config.layer_norm_epsilon
        )
        
        # Decoder
        self.decoder_embed_dropout = nn.Dropout(config.dropout_rate)
        self.decoder_blocks = nn.ModuleList([
            T5DecoderBlock(config, has_relative_attention_bias=(i == 0))
            for i in range(config.num_decoder_layers)
        ])
        self.decoder_final_layer_norm = nn.LayerNorm(
            config.d_model, eps=config.layer_norm_epsilon
        )
        
        # LM head
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
    
    def encode(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode input sequence."""
        hidden_states = self.shared(input_ids)
        hidden_states = self.encoder_embed_dropout(hidden_states)
        
        # Convert attention mask
        if attention_mask is not None:
            extended_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * -1e9
        else:
            extended_mask = None
        
        position_bias = None
        for block in self.encoder_blocks:
            hidden_states, position_bias = block(
                hidden_states,
                attention_mask=extended_mask,
                position_bias=position_bias
            )
        
        hidden_states = self.encoder_final_layer_norm(hidden_states)
        return hidden_states
    
    def decode(
        self,
        decoder_input_ids: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[List]]:
        """Decode output sequence."""
        hidden_states = self.shared(decoder_input_ids)
        hidden_states = self.decoder_embed_dropout(hidden_states)
        
        # Causal mask for decoder
        seq_len = decoder_input_ids.size(1)
        causal_mask = torch.triu(
            torch.ones(seq_len, seq_len, device=decoder_input_ids.device),
            diagonal=1
        ).bool()
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) * -1e9
        
        # Encoder attention mask
        if encoder_attention_mask is not None:
            encoder_extended_mask = (
                1.0 - encoder_attention_mask.unsqueeze(1).unsqueeze(2)
            ) * -1e9
        else:
            encoder_extended_mask = None
        
        position_bias = None
        presents = [] if use_cache else None
        
        for i, block in enumerate(self.decoder_blocks):
            past = past_key_values[i] if past_key_values else None
            
            hidden_states, position_bias, present = block(
                hidden_states,
                encoder_hidden_states,
                attention_mask=causal_mask,
                encoder_attention_mask=encoder_extended_mask,
                position_bias=position_bias,
                past_key_value=past,
                use_cache=use_cache
            )
            
            if use_cache:
                presents.append(present)
        
        hidden_states = self.decoder_final_layer_norm(hidden_states)
        return hidden_states, presents
    
    def forward(
        self,
        input_ids: torch.Tensor,
        decoder_input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Forward pass."""
        # Encode
        encoder_hidden_states = self.encode(input_ids, attention_mask)
        
        # Decode
        decoder_hidden_states, _ = self.decode(
            decoder_input_ids,
            encoder_hidden_states,
            encoder_attention_mask=attention_mask
        )
        
        # LM head
        logits = self.lm_head(decoder_hidden_states)
        
        # Compute loss
        loss = None
        if labels is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
        
        return {'loss': loss, 'logits': logits}


# Example usage
if __name__ == "__main__":
    config = T5Config(vocab_size=32128, d_model=512, num_layers=6, num_heads=8)
    model = T5Model(config)
    
    # Sample input
    input_ids = torch.randint(0, config.vocab_size, (2, 32))
    decoder_input_ids = torch.randint(0, config.vocab_size, (2, 16))
    labels = decoder_input_ids.clone()
    
    outputs = model(input_ids, decoder_input_ids, labels=labels)
    
    print(f"Logits shape: {outputs['logits'].shape}")
    print(f"Loss: {outputs['loss'].item():.4f}")
    print(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")
```

## Task Prefixes

T5 uses text prefixes to specify tasks:

```python
TASK_PREFIXES = {
    "translation_en_to_de": "translate English to German: ",
    "translation_en_to_fr": "translate English to French: ",
    "summarization": "summarize: ",
    "sentiment": "sentiment: ",
    "question": "question: ",
    "answer_span": "answer: ",
}
```

## T5 Variants

| Variant | Key Changes | Significance |
|---------|-------------|--------------|
| **T5 v1.1** | No dropout during pre-training, GEGLU activation, no shared embeddings | Modest but consistent improvements |
| **mT5** | Multilingual, 101 languages, trained on mC4 | Extends text-to-text to cross-lingual tasks |
| **Flan-T5** | Instruction-tuned on 1800+ tasks | Dramatic zero-shot improvement; best open enc-dec model |
| **UL2** | Mixture of Denoisers (span corruption + prefix LM + causal LM) | Unifies pre-training objectives |
| **LongT5** | Adds local + transient global attention | Handles sequences up to 16K tokens |

### T5's Legacy

T5's systematic exploration of Transformer design choices remains one of the most comprehensive ablation studies in the field. Key findings include:

- Encoder-decoder outperforms decoder-only at the same compute budget for supervised tasks
- Pre-norm (layer normalization before sublayers) trains more stably than post-norm
- Span corruption is more parameter-efficient than token-level masking or language modeling
- Larger models are more sample-efficient: doubling parameters while halving data often improves performance

## Summary

T5's unified text-to-text framework:

1. **Simplicity**: One model for all NLP tasks
2. **Scalability**: Works from 60M to 11B parameters
3. **Transfer Learning**: Pre-training transfers across tasks
4. **Flexibility**: Easy to add new tasks via prompts

## References

1. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with T5." JMLR.
2. Chung, H., et al. (2022). "Scaling Instruction-Finetuned Language Models."
3. Tay, Y., et al. (2022). "UL2: Unifying Language Learning Paradigms."

---

## Pre-training Objectives: A Comprehensive Survey

#### Causal Language Modeling (CLM)

Used by GPT, LLaMA, and decoder-only models.

$$
\mathcal{L}_{\text{CLM}} = -\sum_{t=1}^{T} \log P(x_t | x_1, \ldots, x_{t-1})
$$

**Characteristics:**
- Unidirectional (left-to-right)
- Natural for generation
- Simple training setup

```python
import torch
import torch.nn.functional as F

def causal_lm_loss(logits, labels):
    """
    Causal language modeling loss.
    
    Args:
        logits: [batch, seq_len, vocab_size]
        labels: [batch, seq_len]
    """
    # Shift for next-token prediction
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    return F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
        ignore_index=-100
    )
```

#### Masked Language Modeling (MLM)

Used by BERT, RoBERTa, and encoder-only models.

$$
\mathcal{L}_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log P(x_i | \mathbf{x}_{\backslash\mathcal{M}})
$$

**Masking strategy (BERT):**
- 15% of tokens selected
- 80% replaced with [MASK]
- 10% replaced with random token
- 10% kept unchanged

```python
import torch
import torch.nn as nn

def create_mlm_data(input_ids, vocab_size, mask_token_id, mask_prob=0.15):
    """Create MLM training data."""
    labels = input_ids.clone()
    
    # Create mask
    probability_matrix = torch.full(input_ids.shape, mask_prob)
    masked_indices = torch.bernoulli(probability_matrix).bool()
    
    # Don't mask special tokens
    labels[~masked_indices] = -100
    
    # 80% [MASK]
    indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices
    input_ids[indices_replaced] = mask_token_id
    
    # 10% random
    indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced
    random_words = torch.randint(vocab_size, input_ids.shape)
    input_ids[indices_random] = random_words[indices_random]
    
    # 10% unchanged
    return input_ids, labels
```

#### Span Corruption (T5)

Used by T5 and encoder-decoder models.

Replace consecutive spans with sentinel tokens:

```
Input:  "Thank you <X> to your party <Y> week."
Target: "<X> for inviting me <Y> last"
```

```python
def create_span_corruption_data(input_ids, sentinel_start_id, mean_span_length=3, noise_density=0.15):
    """Create span corruption training data (simplified)."""
    seq_len = input_ids.size(1)
    num_masked = int(seq_len * noise_density)
    
    # Select span starts
    num_spans = max(1, num_masked // mean_span_length)
    
    # Create corrupted input and target
    # (Full implementation requires careful span selection)
    pass
```

#### Prefix Language Modeling

Combines bidirectional prefix with causal continuation:

$$
\mathcal{L} = -\sum_{t > L_{\text{prefix}}} \log P(x_t | x_1, \ldots, x_{t-1})
$$

Used in UL2 and some encoder-decoder models.

#### Replaced Token Detection (ELECTRA)

Train a discriminator to detect replaced tokens:

$$
\mathcal{L} = -\sum_{t=1}^{T} \left[ y_t \log D(x_t) + (1-y_t) \log(1 - D(x_t)) \right]
$$

Where $y_t = 1$ if token was replaced by generator.

```python
class ELECTRA(nn.Module):
    def __init__(self, generator, discriminator):
        super().__init__()
        self.generator = generator  # Small MLM model
        self.discriminator = discriminator  # Main model
    
    def forward(self, input_ids, masked_indices):
        # Generator predicts masked tokens
        gen_logits = self.generator(input_ids)
        
        # Sample replacements
        with torch.no_grad():
            gen_probs = F.softmax(gen_logits, dim=-1)
            sampled = torch.multinomial(gen_probs.view(-1, gen_probs.size(-1)), 1)
            sampled = sampled.view(input_ids.shape)
        
        # Replace masked positions
        corrupted = input_ids.clone()
        corrupted[masked_indices] = sampled[masked_indices]
        
        # Discriminator predicts which are replaced
        disc_logits = self.discriminator(corrupted)
        labels = (corrupted != input_ids).float()
        
        return disc_logits, labels
```

#### Denoising Objectives

#### Document Rotation
Rotate document and predict rotation amount.

#### Sentence Permutation
Shuffle sentences and reconstruct order.

#### Token Deletion
Delete tokens and predict original sequence.

#### Comparison

| Objective | Architecture | Bidirectional | Best For |
|-----------|--------------|---------------|----------|
| CLM | Decoder | No | Generation |
| MLM | Encoder | Yes | Understanding |
| Span Corruption | Enc-Dec | Prefix: Yes | Seq2Seq |
| ELECTRA | Encoder | Yes | Efficient pretraining |

#### Mixture of Denoisers (UL2)

Combines multiple objectives:

```python
def ul2_objective(input_ids, mode='R'):
    """
    UL2 mixture of denoisers.
    
    Modes:
    - R: Regular denoising (like T5)
    - S: Sequential denoising (prefix LM)
    - X: Extreme denoising (high corruption)
    """
    if mode == 'R':
        # 15% corruption, mean span 3
        return span_corruption(input_ids, 0.15, 3)
    elif mode == 'S':
        # Prefix LM style
        return prefix_lm(input_ids)
    elif mode == 'X':
        # 50% corruption, mean span 32
        return span_corruption(input_ids, 0.50, 32)
```

#### Summary

Pre-training objectives shape model capabilities:

1. **CLM**: Best for generation, simple training
2. **MLM**: Best for understanding, bidirectional
3. **Span Corruption**: Efficient, good for seq2seq
4. **ELECTRA**: Sample efficient, strong discriminative
5. **UL2**: Versatile, combines multiple objectives

#### References

1. Devlin, J., et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers."
2. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with T5."
3. Clark, K., et al. (2020). "ELECTRA: Pre-training Text Encoders as Discriminators."
4. Tay, Y., et al. (2022). "UL2: Unifying Language Learning Paradigms."
