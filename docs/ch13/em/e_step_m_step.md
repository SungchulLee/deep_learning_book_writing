# E-Step and M-Step

The Expectation-Maximization algorithm derives its name from its two alternating steps. This section provides a detailed examination of each step: the computational mechanics, closed-form derivations for common models, and practical implementation considerations.

---

## E-Step: Computing Expected Sufficient Statistics

### The Fundamental Task

The E-step computes the **posterior distribution** over latent variables given the current parameter estimate:

$$
p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})
$$

From this posterior, we extract the **expected sufficient statistics** needed for the M-step. The key insight is that we don't always need the full posterior—often only certain expectations are required.

### Posterior Computation via Bayes' Theorem

The posterior follows directly from Bayes' theorem:

$$
p(\mathbf{Z} | \mathbf{X}, \theta^{(t)}) = \frac{p(\mathbf{X} | \mathbf{Z}, \theta^{(t)}) \, p(\mathbf{Z} | \theta^{(t)})}{p(\mathbf{X} | \theta^{(t)})}
$$

The denominator is the marginal likelihood:

$$
p(\mathbf{X} | \theta^{(t)}) = \int p(\mathbf{X} | \mathbf{Z}, \theta^{(t)}) \, p(\mathbf{Z} | \theta^{(t)}) \, d\mathbf{Z}
$$

For discrete latent variables, this becomes a sum over all possible configurations.

### Expected Sufficient Statistics

For exponential family models, the E-step reduces to computing:

$$
\bar{T} = \mathbb{E}_{p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})}[T(\mathbf{X}, \mathbf{Z})]
$$

where $T(\mathbf{X}, \mathbf{Z})$ is the sufficient statistic of the complete-data distribution. This expectation is often much simpler to compute than the full posterior.

### Independence Structure

When data points are independent, the joint posterior factorizes:

$$
p(\mathbf{Z} | \mathbf{X}, \theta) = \prod_{i=1}^{N} p(z_i | \mathbf{x}_i, \theta)
$$

This reduces the E-step to $N$ independent computations, enabling parallelization and tractability.

### Responsibilities in Mixture Models

For mixture models with $K$ components, the E-step computes **responsibilities**:

$$
\gamma_{ik} = p(z_i = k | \mathbf{x}_i, \theta^{(t)}) = \frac{p(z_i = k | \theta^{(t)}) \, p(\mathbf{x}_i | z_i = k, \theta^{(t)})}{\sum_{j=1}^{K} p(z_i = j | \theta^{(t)}) \, p(\mathbf{x}_i | z_i = j, \theta^{(t)})}
$$

These responsibilities represent the **soft assignment** of each observation to each component and satisfy:

- $0 \leq \gamma_{ik} \leq 1$ for all $i, k$
- $\sum_{k=1}^{K} \gamma_{ik} = 1$ for each observation $i$

### E-Step Complexity

The computational cost of the E-step depends on the latent variable structure:

| Model | Latent Structure | E-Step Complexity |
|-------|-----------------|-------------------|
| Gaussian Mixture | Discrete, independent | $O(NK)$ |
| Hidden Markov Model | Discrete, sequential | $O(NK^2T)$ via forward-backward |
| Factor Analysis | Continuous Gaussian | $O(Nd^3)$ where $d$ is latent dimension |
| Latent Dirichlet Allocation | Discrete, coupled | Approximate inference required |

---

## E-Step Derivations for Common Models

### Gaussian Mixture Model

**Model**: $K$ Gaussian components with mixing proportions $\boldsymbol{\pi} = (\pi_1, \ldots, \pi_K)$, means $\{\boldsymbol{\mu}_k\}$, and covariances $\{\boldsymbol{\Sigma}_k\}$.

**Latent variable**: $z_i \in \{1, \ldots, K\}$ indicates component membership.

**E-step derivation**:

$$
\gamma_{ik} = \frac{\pi_k \, \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \, \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
$$

where the Gaussian density is:

$$
\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right)
$$

**Expected sufficient statistics**:

$$
N_k = \sum_{i=1}^{N} \gamma_{ik}, \quad \bar{\mathbf{x}}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i, \quad \bar{S}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \bar{\mathbf{x}}_k)(\mathbf{x}_i - \bar{\mathbf{x}}_k)^\top
$$

### Factor Analysis

**Model**: Observations generated by linear transformation of latent factors:

$$
\mathbf{x}_i = \mathbf{W} \mathbf{z}_i + \boldsymbol{\mu} + \boldsymbol{\epsilon}_i, \quad \mathbf{z}_i \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \quad \boldsymbol{\epsilon}_i \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Psi})
$$

where $\mathbf{W}$ is the factor loading matrix and $\boldsymbol{\Psi}$ is diagonal.

**E-step derivation**: The posterior over latents is Gaussian:

$$
p(\mathbf{z}_i | \mathbf{x}_i, \theta) = \mathcal{N}(\mathbf{z}_i | \mathbf{m}_i, \mathbf{V})
$$

with:

$$
\mathbf{V} = (\mathbf{I} + \mathbf{W}^\top \boldsymbol{\Psi}^{-1} \mathbf{W})^{-1}
$$

$$
\mathbf{m}_i = \mathbf{V} \mathbf{W}^\top \boldsymbol{\Psi}^{-1} (\mathbf{x}_i - \boldsymbol{\mu})
$$

**Expected sufficient statistics**:

$$
\mathbb{E}[\mathbf{z}_i | \mathbf{x}_i] = \mathbf{m}_i
$$

$$
\mathbb{E}[\mathbf{z}_i \mathbf{z}_i^\top | \mathbf{x}_i] = \mathbf{V} + \mathbf{m}_i \mathbf{m}_i^\top
$$

### Hidden Markov Model

**Model**: Sequence of latent states $\{z_1, \ldots, z_T\}$ with transition matrix $\mathbf{A}$ and emission distributions $\{p(\mathbf{x}_t | z_t)\}$.

**E-step via Forward-Backward Algorithm**:

**Forward pass** computes $\alpha_t(k) = p(\mathbf{x}_{1:t}, z_t = k)$:

$$
\alpha_1(k) = \pi_k \, p(\mathbf{x}_1 | z_1 = k)
$$

$$
\alpha_t(k) = p(\mathbf{x}_t | z_t = k) \sum_{j=1}^{K} \alpha_{t-1}(j) \, A_{jk}
$$

**Backward pass** computes $\beta_t(k) = p(\mathbf{x}_{t+1:T} | z_t = k)$:

$$
\beta_T(k) = 1
$$

$$
\beta_t(k) = \sum_{j=1}^{K} A_{kj} \, p(\mathbf{x}_{t+1} | z_{t+1} = j) \, \beta_{t+1}(j)
$$

**Expected sufficient statistics**:

$$
\gamma_t(k) = p(z_t = k | \mathbf{x}_{1:T}) = \frac{\alpha_t(k) \beta_t(k)}{\sum_{j} \alpha_t(j) \beta_t(j)}
$$

$$
\xi_t(j, k) = p(z_t = j, z_{t+1} = k | \mathbf{x}_{1:T}) = \frac{\alpha_t(j) A_{jk} p(\mathbf{x}_{t+1} | z_{t+1} = k) \beta_{t+1}(k)}{\sum_{j',k'} \alpha_t(j') A_{j'k'} p(\mathbf{x}_{t+1} | z_{t+1} = k') \beta_{t+1}(k')}
$$

---

## M-Step: Parameter Update Derivations

### The Optimization Problem

The M-step maximizes the **Q-function** (expected complete-data log-likelihood):

$$
\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})
$$

where:

$$
Q(\theta | \theta^{(t)}) = \mathbb{E}_{p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})}[\log p(\mathbf{X}, \mathbf{Z} | \theta)]
$$

### Why the Q-Function is Tractable

The Q-function is easier to optimize than the marginal log-likelihood because:

1. **Log inside expectation**: $\mathbb{E}[\log p(\mathbf{X}, \mathbf{Z} | \theta)]$ rather than $\log \mathbb{E}[p(\mathbf{X}, \mathbf{Z} | \theta)]$
2. **Exponential family structure**: For exponential families, Q is concave in natural parameters
3. **Parameter decoupling**: Different parameter groups often decouple in Q

### Deriving M-Step Updates

The general procedure:

1. Write out $\log p(\mathbf{X}, \mathbf{Z} | \theta)$ explicitly
2. Take expectation with respect to $p(\mathbf{Z} | \mathbf{X}, \theta^{(t)})$
3. Differentiate with respect to each parameter
4. Set gradient to zero and solve

### Complete Data Log-Likelihood Decomposition

For independent observations with factorized likelihood:

$$
\log p(\mathbf{X}, \mathbf{Z} | \theta) = \sum_{i=1}^{N} \log p(\mathbf{x}_i, z_i | \theta) = \sum_{i=1}^{N} \left[ \log p(z_i | \theta) + \log p(\mathbf{x}_i | z_i, \theta) \right]
$$

The Q-function then separates into prior and likelihood terms:

$$
Q(\theta | \theta^{(t)}) = \underbrace{\sum_{i=1}^{N} \mathbb{E}[\log p(z_i | \theta)]}_{\text{prior term}} + \underbrace{\sum_{i=1}^{N} \mathbb{E}[\log p(\mathbf{x}_i | z_i, \theta)]}_{\text{likelihood term}}
$$

---

## M-Step Derivations for Common Models

### Gaussian Mixture Model

**Q-function**:

$$
Q(\theta | \theta^{(t)}) = \sum_{i=1}^{N} \sum_{k=1}^{K} \gamma_{ik} \left[ \log \pi_k + \log \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
$$

Expanding the Gaussian log-density:

$$
Q = \sum_{i,k} \gamma_{ik} \log \pi_k - \frac{1}{2} \sum_{i,k} \gamma_{ik} \left[ d \log(2\pi) + \log|\boldsymbol{\Sigma}_k| + (\mathbf{x}_i - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) \right]
$$

**Mixing proportions** (with constraint $\sum_k \pi_k = 1$):

Using Lagrange multipliers:

$$
\frac{\partial}{\partial \pi_k} \left[ Q + \lambda \left( \sum_j \pi_j - 1 \right) \right] = \frac{N_k}{\pi_k} + \lambda = 0
$$

Solving: $\pi_k = -N_k / \lambda$. The constraint gives $\lambda = -N$, thus:

$$
\boxed{\pi_k^{(t+1)} = \frac{N_k}{N} = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}}
$$

**Means**:

$$
\frac{\partial Q}{\partial \boldsymbol{\mu}_k} = \sum_{i=1}^{N} \gamma_{ik} \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) = 0
$$

Solving:

$$
\boxed{\boldsymbol{\mu}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i}{\sum_{i=1}^{N} \gamma_{ik}} = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i}
$$

**Covariances**:

Using matrix calculus ($\frac{\partial}{\partial \boldsymbol{\Sigma}^{-1}} \log|\boldsymbol{\Sigma}^{-1}| = \boldsymbol{\Sigma}$):

$$
\frac{\partial Q}{\partial \boldsymbol{\Sigma}_k^{-1}} = \frac{N_k}{2} \boldsymbol{\Sigma}_k - \frac{1}{2} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k)(\mathbf{x}_i - \boldsymbol{\mu}_k)^\top = 0
$$

Solving:

$$
\boxed{\boldsymbol{\Sigma}_k^{(t+1)} = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k^{(t+1)})(\mathbf{x}_i - \boldsymbol{\mu}_k^{(t+1)})^\top}
$$

### Factor Analysis

**Q-function** (dropping constants):

$$
Q = -\frac{N}{2} \log|\boldsymbol{\Psi}| - \frac{1}{2} \sum_{i=1}^{N} \text{tr}\left( \boldsymbol{\Psi}^{-1} \mathbb{E}\left[ (\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{W}\mathbf{z}_i)(\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{W}\mathbf{z}_i)^\top \right] \right)
$$

**Loading matrix** $\mathbf{W}$:

$$
\frac{\partial Q}{\partial \mathbf{W}} = \boldsymbol{\Psi}^{-1} \sum_{i=1}^{N} \left[ (\mathbf{x}_i - \boldsymbol{\mu}) \mathbb{E}[\mathbf{z}_i]^\top - \mathbf{W} \mathbb{E}[\mathbf{z}_i \mathbf{z}_i^\top] \right] = 0
$$

Solving:

$$
\boxed{\mathbf{W}^{(t+1)} = \left( \sum_{i=1}^{N} (\mathbf{x}_i - \boldsymbol{\mu}) \mathbf{m}_i^\top \right) \left( \sum_{i=1}^{N} (\mathbf{V} + \mathbf{m}_i \mathbf{m}_i^\top) \right)^{-1}}
$$

**Noise variance** $\boldsymbol{\Psi}$ (diagonal elements):

$$
\boxed{\Psi_{jj}^{(t+1)} = \frac{1}{N} \sum_{i=1}^{N} \left[ (x_{ij} - \mu_j)^2 - 2 W_j^{(t+1)} m_i (x_{ij} - \mu_j) + W_j^{(t+1)} (\mathbf{V} + \mathbf{m}_i \mathbf{m}_i^\top) W_j^{(t+1)\top} \right]}
$$

where $W_j$ is the $j$-th row of $\mathbf{W}$.

### Hidden Markov Model

**Q-function**:

$$
Q = \sum_{k=1}^{K} \gamma_1(k) \log \pi_k + \sum_{t=1}^{T-1} \sum_{j,k} \xi_t(j,k) \log A_{jk} + \sum_{t=1}^{T} \sum_{k=1}^{K} \gamma_t(k) \log p(\mathbf{x}_t | z_t = k, \theta)
$$

**Initial distribution**:

$$
\boxed{\pi_k^{(t+1)} = \gamma_1(k)}
$$

**Transition matrix**:

$$
\boxed{A_{jk}^{(t+1)} = \frac{\sum_{t=1}^{T-1} \xi_t(j,k)}{\sum_{t=1}^{T-1} \gamma_t(j)}}
$$

**Emission parameters**: Depend on emission distribution family (Gaussian, multinomial, etc.), updated using responsibilities $\gamma_t(k)$ as weights.

---

## Numerical Stability Considerations

### Log-Space Computation

Direct computation of responsibilities can underflow:

$$
\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
$$

**Solution**: Compute in log-space using the log-sum-exp trick:

$$
\log \gamma_{ik} = \log \pi_k + \log \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) - \text{LogSumExp}_j \left( \log \pi_j + \log \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j) \right)
$$

where:

$$
\text{LogSumExp}(a_1, \ldots, a_K) = a_{\max} + \log \sum_{k=1}^{K} \exp(a_k - a_{\max})
$$

### Covariance Regularization

Singular covariances cause numerical issues. Common fixes:

1. **Diagonal loading**: $\boldsymbol{\Sigma}_k \leftarrow \boldsymbol{\Sigma}_k + \epsilon \mathbf{I}$
2. **Minimum eigenvalue**: Clamp eigenvalues above threshold
3. **Tied covariances**: Share $\boldsymbol{\Sigma}$ across components
4. **Diagonal covariances**: Restrict to $\boldsymbol{\Sigma}_k = \text{diag}(\sigma_{k1}^2, \ldots, \sigma_{kd}^2)$

### Component Collapse Prevention

When $N_k \to 0$ (empty component):

1. **Reinitialize**: Reset component to random data point
2. **Merge and split**: Merge empty with largest, split largest
3. **Regularization**: Add prior counts (Bayesian approach)

---

## PyTorch Implementation

```python
import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal, Categorical

class GaussianMixtureEM:
    """Gaussian Mixture Model with EM algorithm."""
    
    def __init__(self, n_components: int, n_features: int, 
                 covariance_type: str = 'full', reg_covar: float = 1e-6):
        self.K = n_components
        self.d = n_features
        self.covariance_type = covariance_type
        self.reg_covar = reg_covar
        
        # Initialize parameters
        self.pi = torch.ones(self.K) / self.K  # Mixing proportions
        self.mu = torch.randn(self.K, self.d)   # Means
        self.Sigma = torch.stack([torch.eye(self.d) for _ in range(self.K)])  # Covariances
        
    def e_step(self, X: torch.Tensor) -> torch.Tensor:
        """
        E-step: Compute responsibilities.
        
        Args:
            X: Data tensor of shape (N, d)
            
        Returns:
            gamma: Responsibilities of shape (N, K)
        """
        N = X.shape[0]
        log_resp = torch.zeros(N, self.K)
        
        for k in range(self.K):
            mvn = MultivariateNormal(self.mu[k], self.Sigma[k])
            log_resp[:, k] = torch.log(self.pi[k]) + mvn.log_prob(X)
        
        # Log-sum-exp for numerical stability
        log_resp_sum = torch.logsumexp(log_resp, dim=1, keepdim=True)
        log_gamma = log_resp - log_resp_sum
        gamma = torch.exp(log_gamma)
        
        return gamma
    
    def m_step(self, X: torch.Tensor, gamma: torch.Tensor):
        """
        M-step: Update parameters.
        
        Args:
            X: Data tensor of shape (N, d)
            gamma: Responsibilities of shape (N, K)
        """
        N = X.shape[0]
        
        # Effective counts
        N_k = gamma.sum(dim=0)  # (K,)
        
        # Update mixing proportions
        self.pi = N_k / N
        
        # Update means
        self.mu = (gamma.T @ X) / N_k.unsqueeze(1)  # (K, d)
        
        # Update covariances
        for k in range(self.K):
            diff = X - self.mu[k]  # (N, d)
            weighted_diff = gamma[:, k].unsqueeze(1) * diff  # (N, d)
            self.Sigma[k] = (weighted_diff.T @ diff) / N_k[k]
            
            # Regularization
            self.Sigma[k] += self.reg_covar * torch.eye(self.d)
    
    def log_likelihood(self, X: torch.Tensor) -> float:
        """Compute log-likelihood of data."""
        N = X.shape[0]
        log_prob = torch.zeros(N, self.K)
        
        for k in range(self.K):
            mvn = MultivariateNormal(self.mu[k], self.Sigma[k])
            log_prob[:, k] = torch.log(self.pi[k]) + mvn.log_prob(X)
        
        return torch.logsumexp(log_prob, dim=1).sum().item()
    
    def fit(self, X: torch.Tensor, max_iters: int = 100, 
            tol: float = 1e-4, verbose: bool = False):
        """
        Fit GMM using EM algorithm.
        
        Args:
            X: Data tensor of shape (N, d)
            max_iters: Maximum number of iterations
            tol: Convergence tolerance
            verbose: Print progress
        """
        prev_ll = float('-inf')
        
        for iteration in range(max_iters):
            # E-step
            gamma = self.e_step(X)
            
            # M-step
            self.m_step(X, gamma)
            
            # Check convergence
            ll = self.log_likelihood(X)
            
            if verbose and iteration % 10 == 0:
                print(f"Iteration {iteration}: log-likelihood = {ll:.4f}")
            
            if abs(ll - prev_ll) < tol:
                if verbose:
                    print(f"Converged at iteration {iteration}")
                break
            
            prev_ll = ll
        
        return self
    
    def predict(self, X: torch.Tensor) -> torch.Tensor:
        """Predict cluster assignments."""
        gamma = self.e_step(X)
        return gamma.argmax(dim=1)
    
    def predict_proba(self, X: torch.Tensor) -> torch.Tensor:
        """Predict cluster probabilities (responsibilities)."""
        return self.e_step(X)


# Example usage
if __name__ == "__main__":
    torch.manual_seed(42)
    
    # Generate synthetic data from 3-component GMM
    N = 300
    true_means = torch.tensor([[0., 0.], [3., 3.], [-2., 3.]])
    
    X = torch.cat([
        torch.randn(100, 2) + true_means[0],
        torch.randn(100, 2) + true_means[1],
        torch.randn(100, 2) + true_means[2]
    ])
    
    # Fit GMM
    gmm = GaussianMixtureEM(n_components=3, n_features=2)
    gmm.fit(X, verbose=True)
    
    print(f"\nLearned means:\n{gmm.mu}")
    print(f"\nLearned mixing proportions: {gmm.pi}")
```

---

## Summary

| Step | Input | Output | Computation |
|------|-------|--------|-------------|
| **E-step** | Current $\theta^{(t)}$, data $\mathbf{X}$ | Responsibilities $\gamma$ or posterior moments | Bayes' theorem, forward-backward |
| **M-step** | Responsibilities $\gamma$, data $\mathbf{X}$ | Updated $\theta^{(t+1)}$ | Weighted MLE, closed-form or optimization |

### Key Insights

1. **E-step simplification**: Often only expected sufficient statistics are needed, not the full posterior
2. **M-step tractability**: Exponential family structure yields closed-form updates
3. **Numerical stability**: Log-space computation and regularization are essential in practice
4. **Independence**: Factorization over data points enables efficient, parallelizable implementation

The alternation between these steps—computing what we expect the latent variables to be (E) and then optimizing parameters as if those expectations were true (M)—is the elegant core of the EM algorithm.
