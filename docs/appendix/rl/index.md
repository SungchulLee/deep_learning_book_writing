# A9 Reinforcement Learning

## Overview

This appendix provides complete PyTorch implementations of deep reinforcement learning (RL) algorithms spanning value-based, policy gradient, and actor–critic methods. RL agents learn optimal decision-making policies through interaction with an environment, maximizing cumulative reward. In quantitative finance, RL is applied to dynamic portfolio allocation, optimal trade execution, market making, and hedging strategies where sequential decision-making under uncertainty is fundamental.

## Algorithms

### Value-Based Methods

| Algorithm | Year | Key Innovation |
|-----------|------|----------------|
| [DQN](dqn.py) | 2015 | Deep Q-network with experience replay and target networks |
| [Double DQN](double_dqn.py) | 2016 | Decoupled action selection and evaluation to reduce overestimation bias |
| [Dueling DQN](dueling_dqn.py) | 2016 | Separate value and advantage streams for better state evaluation |

### Actor–Critic Methods

| Algorithm | Year | Key Innovation |
|-----------|------|----------------|
| [A2C/A3C](a2c_a3c.py) | 2016 | Advantage actor–critic with parallel environment workers |
| [PPO](ppo.py) | 2017 | Clipped surrogate objective for stable policy updates |
| [SAC](sac.py) | 2018 | Maximum entropy RL with automatic temperature tuning |
| [TD3](td3.py) | 2018 | Twin critics, delayed policy updates, target policy smoothing |

## Key Concepts

### The RL Framework

An agent interacts with an environment modeled as a Markov Decision Process (MDP):

- **State** $s_t$: Current market/portfolio representation
- **Action** $a_t$: Trading decision (buy, sell, hold, allocation weights)
- **Reward** $r_t$: Risk-adjusted return, Sharpe ratio increment, or transaction cost-adjusted P&L
- **Policy** $\pi(a|s)$: Mapping from states to action probabilities
- **Value function** $V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid s_t = s\right]$

### Value-Based vs. Policy Gradient

| Aspect | Value-Based (DQN) | Policy Gradient (PPO, SAC) |
|--------|-------------------|---------------------------|
| Action space | Discrete | Continuous or discrete |
| Exploration | $\epsilon$-greedy | Stochastic policy / entropy bonus |
| Stability | Target networks, replay buffer | Clipping, trust regions |
| Sample efficiency | Higher (off-policy replay) | Lower (on-policy) or moderate (off-policy SAC) |

### Stability Techniques

- **Experience replay**: Store and sample past transitions to break temporal correlations (DQN, SAC, TD3)
- **Target networks**: Slowly updated copies of value networks to stabilize training (DQN, TD3)
- **Clipped objectives**: PPO's clipped surrogate prevents destructively large policy updates
- **Entropy regularization**: SAC maximizes expected reward plus entropy for robust exploration
- **Twin critics**: TD3 and SAC use two Q-networks, taking the minimum to reduce overestimation

### On-Policy vs. Off-Policy

- **On-policy** (A2C, PPO): Learn from data generated by the current policy; discard data after each update
- **Off-policy** (DQN, SAC, TD3): Learn from stored transitions regardless of the generating policy; more sample efficient

## Quantitative Finance Applications

- **Portfolio optimization**: PPO/SAC for dynamic multi-asset allocation with transaction costs and risk constraints
- **Optimal execution**: Minimize market impact when liquidating large positions over time (DQN, TD3)
- **Market making**: Learn bid–ask spread strategies that balance inventory risk and profit (SAC)
- **Hedging**: RL-based delta hedging under transaction costs outperforms Black–Scholes discrete hedging
- **Pairs trading**: Learn entry/exit signals for statistical arbitrage strategies
- **Order routing**: Optimize order placement across venues to minimize execution costs

## Prerequisites

- [Ch3: Neural Network Fundamentals](../../ch03/index.md) — MLP architectures for value and policy networks
- [A10: Utility Modules — Activation Functions](../utils/activations.py) — tanh for bounded continuous actions
- [A10: Utility Modules — Loss Functions](../utils/losses.py) — Huber loss for DQN, policy gradient losses
- [A10: Utility Modules — Learning Rate Schedulers](../utils/schedulers.py) — warm-up and decay for stable RL training
