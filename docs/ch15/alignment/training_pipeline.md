# 6.11.4 Training Process

The training of ChatGPT proceeds in two major phases — **pre-training** and **alignment** — each with distinct objectives, data requirements, and optimization strategies.

## Phase 1: Pre-Training

### Objective

During pre-training, the model learns general language representations by training on a large-scale text corpus using the **causal language modeling (CLM)** objective. Given a sequence of tokens $\mathbf{x} = (x_1, x_2, \ldots, x_T)$, the model is trained to maximize the log-likelihood:

$$
\mathcal{L}_{\text{CLM}}(\theta) = \sum_{t=1}^{T} \log p_\theta(x_t \mid x_1, \ldots, x_{t-1})
$$

This is equivalent to minimizing the cross-entropy between the model's predicted distribution and the empirical next-token distribution. The model learns to capture patterns in grammar, facts, reasoning structures, and stylistic conventions from the training data.

!!! warning "Clarification on Masked LM vs. Causal LM"
    GPT models use **causal (left-to-right) language modeling**, not masked language modeling (MLM). MLM, where random tokens are masked and predicted from bidirectional context, is the approach used by BERT-family models. The GPT pre-training objective always predicts the *next* token conditioned on all *preceding* tokens. This distinction is important because the causal structure is what enables autoregressive generation at inference time.

### Training Data

GPT models are pre-trained on massive, diverse text corpora. While exact dataset compositions vary by model version, typical sources include:

- **Web crawl data** (filtered Common Crawl) — the largest component by volume
- **Books** (fiction and non-fiction corpora)
- **Wikipedia** and other reference sources
- **Code repositories** (especially for later models)
- **Academic papers and technical documentation**

Data quality is crucial. Raw web data contains noise, duplication, and harmful content, so extensive preprocessing is applied: deduplication (both exact and fuzzy), quality filtering (using classifier scores trained on high-quality reference text), and content filtering to remove toxic or personally identifiable information.

### Optimization

Pre-training uses the **AdamW** optimizer (Loshchilov & Hutter, 2019) with the following typical hyperparameter ranges:

$$
\theta_{t+1} = \theta_t - \eta_t \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \, \theta_t \right)
$$

where $\hat{m}_t$ and $\hat{v}_t$ are the bias-corrected first and second moment estimates, $\eta_t$ is the learning rate at step $t$ (following a cosine or linear warmup-then-decay schedule), and $\lambda$ is the weight decay coefficient.

Key training details for large GPT models:

- **Batch size:** Gradually increased during training (e.g., from 32K to 3.2M tokens per batch for GPT-3)
- **Learning rate:** Warmup over the first ~375M tokens, then cosine decay
- **Context length:** 2,048 tokens (GPT-3) or longer for later models
- **Mixed precision:** FP16/BF16 training with loss scaling for numerical stability
- **Distributed training:** Model parallelism (tensor + pipeline) across hundreds or thousands of GPUs

!!! info "Compute Scale"
    GPT-3 (175B parameters) required approximately 3,640 petaflop/s-days of compute. At current hardware prices, this represents millions of dollars in training cost — a key reason why only a small number of organizations can train frontier language models from scratch. See [Section 6.1: Scaling Laws](../language_models/fundamentals.md) for the compute-optimal training analysis.

## Phase 2: Alignment

Pre-training produces a model that is capable of generating fluent text but is not inherently aligned with human preferences for helpfulness, safety, and instruction-following. The alignment phase addresses this through three sequential steps.

### Step 1: Supervised Fine-Tuning (SFT)

Human annotators write high-quality responses to a diverse set of prompts, creating a dataset of $(x, y)$ pairs where $x$ is a prompt and $y$ is the desired response. The pre-trained model is fine-tuned on this demonstration data using the same CLM objective:

$$
\mathcal{L}_{\text{SFT}}(\theta) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{\text{demo}}} \left[ \sum_{\ell=1}^{|y|} \log p_\theta(y_\ell \mid x, y_{<\ell}) \right]
$$

The SFT stage typically uses a small learning rate (e.g., $10^{-5}$) and trains for only a few epochs to avoid overfitting to the relatively small demonstration dataset.

### Step 2: Reward Model (RM) Training

A separate model $R_\phi(x, y)$ is trained to predict human preferences over responses. Human annotators are shown a prompt $x$ and several candidate responses $y_1, \ldots, y_K$ generated by the SFT model, then asked to rank them by quality.

These rankings are converted to pairwise comparisons and used to train the reward model via the **Bradley-Terry** preference model:

$$
\mathcal{L}_{\text{RM}}(\phi) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}_{\text{rank}}} \Big[ \log \sigma\!\big(R_\phi(x, y_w) - R_\phi(x, y_l)\big) \Big]
$$

where $(y_w, y_l)$ is a (preferred, dispreferred) response pair and $\sigma$ is the sigmoid function. The reward model is typically initialized from the SFT model (with the language modeling head replaced by a scalar output head) to leverage the learned representations.

!!! note "Why Pairwise Comparisons?"
    Absolute quality ratings are noisy and inconsistent across annotators. Pairwise comparisons ("Is response A better than B?") are more reliable and produce more consistent reward signals. The Bradley-Terry model provides a principled way to aggregate pairwise comparisons into a scalar reward function.

### Step 3: Reinforcement Learning from Human Feedback (RLHF)

The SFT model is further optimized to maximize the learned reward using **Proximal Policy Optimization (PPO)** (Schulman et al., 2017). The optimization objective is:

$$
\max_\theta \; \mathbb{E}_{x \sim \mathcal{D},\; y \sim \pi_\theta(\cdot \mid x)} \Big[ R_\phi(x, y) \Big] - \beta \, \mathbb{E}_{x \sim \mathcal{D}} \Big[ D_{\text{KL}}\!\big(\pi_\theta(\cdot \mid x) \;\|\; \pi_{\text{SFT}}(\cdot \mid x)\big) \Big]
$$

The **KL divergence penalty** (weighted by $\beta > 0$) is critical: it prevents the policy from diverging too far from the SFT model, which would lead to **reward hacking** — generating outputs that achieve high reward scores without genuinely useful content.

The PPO update uses a clipped surrogate objective to ensure stable policy updates:

$$
\mathcal{L}_{\text{PPO}}(\theta) = \mathbb{E}_t \Big[ \min\!\big(r_t(\theta) \hat{A}_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\big) \Big]
$$

where $r_t(\theta) = \pi_\theta(a_t \mid s_t) / \pi_{\theta_{\text{old}}}(a_t \mid s_t)$ is the probability ratio and $\hat{A}_t$ is the estimated advantage. In the language model setting, "actions" are token selections and "states" are the generated token prefixes.

!!! tip "Practical Consideration"
    The RLHF phase is computationally expensive, requiring simultaneous inference from the policy model, the reference (SFT) model, the reward model, and the value model (for advantage estimation). This effectively quadruples the memory requirements compared to standard fine-tuning.

## Bias Mitigation During Training

Bias mitigation is an ongoing concern throughout both pre-training and alignment:

**Data-level interventions** include filtering training data to remove or downweight content containing harmful stereotypes, slurs, or personally identifiable information. Deduplication also reduces the amplification of biases present in overrepresented sources.

**Model-level interventions** during RLHF involve training the reward model on comparison data that specifically penalizes biased or stereotypical outputs. Annotator guidelines explicitly instruct raters to flag and downrank responses exhibiting gender, racial, or cultural bias.

**Post-training interventions** include rule-based output filtering, safety classifiers that flag potentially harmful responses before they reach the user, and red-teaming exercises where adversarial testers deliberately probe the model for failure modes.

!!! note "Cross-Reference"
    For a comprehensive treatment of bias and fairness in machine learning, including formal definitions of fairness criteria (demographic parity, equalized odds, calibration) and debiasing techniques, see [Chapter 30: Bias and Fairness](../../ch30/index.md).

---

**Next:** [6.11.5 Applications of ChatGPT](applications.md)
