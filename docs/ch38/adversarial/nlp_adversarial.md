# Adversarial Attacks on NLP Models

## Introduction

While adversarial robustness research began in the image domain (FGSM, PGD, C&W), the rise of NLP-based financial models — sentiment analysis for trading signals, news-driven event detection, earnings call analysis — makes text-domain adversarial attacks directly relevant to quantitative finance. Unlike images where perturbations are continuous, text attacks must operate in a discrete space, replacing words or characters while preserving semantic meaning and grammatical correctness.

## Taxonomy of NLP Adversarial Attacks

### Character-Level Attacks

The simplest attacks introduce typographical perturbations — swapping characters, inserting whitespace, or using homoglyphs (visually similar Unicode characters). While crude, these attacks exploit the fact that many NLP pipelines use subword tokenization sensitive to exact spelling:

$$
\text{"profitable"} \rightarrow \text{"prof1table"} \quad (\text{homoglyph substitution})
$$

**Financial relevance**: Automated text processing of filings, news feeds, or social media can be disrupted by character-level noise, especially in scraped data from unstructured sources.

### Word-Level Attacks

More sophisticated attacks replace entire words with semantically similar alternatives that flip the model's prediction. The key challenge is maintaining the original meaning while changing the classification:

$$
x_{\text{adv}} = \text{Replace}(x, w_i \rightarrow w_i', \; i \in S)
$$

where $S$ is a selected subset of word positions and each replacement $w_i'$ is chosen to be semantically close to $w_i$ (via word embedding distance or synonym sets) while maximally changing the model's output.

### Sentence-Level Attacks

These attacks paraphrase entire sentences or insert adversarial distractors. They are harder to detect but also harder to construct automatically. Examples include appending irrelevant but confident-sounding sentences to shift model predictions.

## TextFooler: A Word-Level Black-Box Attack

TextFooler (Jin et al., 2020) is a representative word-level attack that operates in a black-box setting — the attacker can query the target model for confidence scores but has no access to model parameters or gradients.

### Algorithm Overview

The attack proceeds in two stages:

**Stage 1 — Word Importance Ranking**: For each word $w_i$ in the input, compute its importance by measuring the change in prediction confidence when $w_i$ is removed:

$$
I(w_i) = P(y \mid x) - P(y \mid x_{\setminus i})
$$

Words are sorted by descending importance, prioritizing those whose removal most affects the prediction.

**Stage 2 — Word Replacement**: For each word in importance order, find candidate replacements from a counter-fitted embedding space (where synonyms are close and antonyms are far). Filter candidates to ensure:

1. **Semantic similarity**: The replacement $w_i'$ must be close to $w_i$ in embedding space
2. **Part-of-speech consistency**: $w_i'$ must share the same POS tag as $w_i$
3. **Sentence similarity**: The overall sentence embedding must remain similar (measured by Universal Sentence Encoder)

Select the replacement that most reduces the target class confidence. Stop when the prediction flips or all important words have been tried.

### Empirical Results

On a BERT model fine-tuned for movie review sentiment classification (MR dataset):

| Metric | Value |
|--------|-------|
| Original accuracy | 90.4% |
| Accuracy after attack | 79.7% |
| Average words changed | 15.3% |
| Average queries per attack | 53.2 |

The attack successfully flips ~10.7% of correctly classified samples by changing only ~15% of words per sentence on average. Importantly, the adversarial sentences remain semantically similar to humans — a reviewer reading both versions would assign the same sentiment.

## Defenses for NLP Models

### Adversarial Training

Augment the training set with adversarial examples generated by the attack itself:

$$
\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \mathcal{L}(f_\theta(x), y) + \lambda \cdot \mathcal{L}(f_\theta(x_{\text{adv}}), y) \right]
$$

This is computationally expensive for text models due to the discrete search required to generate each adversarial example.

### Certified Robustness via Randomized Smoothing

Randomly delete or replace words at inference time, then aggregate predictions across multiple corrupted versions. If the majority prediction is consistent, the model is certifiably robust to a bounded number of word substitutions.

### Input Preprocessing

Apply spell-checking, synonym normalization, or paraphrase detection before feeding text to the model. This reduces the attack surface but may alter legitimate inputs.

### Ensemble Methods

Combine predictions from models using different tokenization strategies (word-level, character-level, subword) or different architectures. Adversarial examples that fool one tokenization scheme often fail against others.

## Implications for Quantitative Finance

NLP adversarial attacks pose specific risks in financial applications:

### Sentiment Analysis Manipulation

Trading strategies that rely on news sentiment scores are vulnerable. An adversary could craft social media posts or forum comments that appear neutral to human readers but trigger strong buy/sell signals in automated sentiment models. This is especially concerning for strategies consuming unfiltered social media data.

### Earnings Call Analysis

Automated analysis of earnings transcripts can be manipulated by carefully choosing words that are semantically equivalent to humans but change model-extracted sentiment. A company's IR team could inadvertently (or deliberately) use language that games automated analysis systems.

### Document Classification

Regulatory filings, loan applications, or insurance claims processed by NLP models can be crafted to influence classification outcomes while appearing legitimate to human reviewers.

### Defense Recommendations for Financial NLP

1. **Multi-model consensus**: Never rely on a single NLP model for trading decisions. Require agreement across architecturally diverse models (transformer, RNN, traditional ML with TF-IDF features).
2. **Input auditing**: Log and monitor unusual word choices or patterns in high-stakes text inputs. Flag documents whose word distributions deviate from expected baselines.
3. **Confidence calibration**: Treat low-confidence predictions as abstentions rather than forcing a classification. Adversarial examples often produce predictions near the decision boundary.
4. **Human-in-the-loop**: For material trading decisions triggered by NLP signals, maintain human review for edge cases where model confidence is below a threshold.
5. **Regular red-teaming**: Periodically test financial NLP models with adversarial attacks to identify vulnerabilities before adversaries do.

## Key Takeaways

1. NLP adversarial attacks operate in discrete space, making them qualitatively different from image-domain attacks but equally dangerous.
2. Black-box attacks like TextFooler require only query access — no model internals — making them practical threats for any deployed API.
3. Word-level attacks can flip BERT predictions by changing ~15% of words while preserving human-perceived meaning.
4. Financial NLP models are attractive targets because small prediction changes can trigger large capital allocation decisions.
5. Defense requires a combination of adversarial training, ensemble methods, and operational safeguards.

## References

1. Jin, D., Jin, Z., Zhou, J. T., & Smeaton, P. (2020). "Is BERT Really Robust? A Strong Baseline for Natural Language Attack and Defense." *AAAI*.
2. Alzantot, M., et al. (2018). "Generating Natural Language Adversarial Examples." *EMNLP*.
3. Ribeiro, M. T., Singh, S., & Guestrin, C. (2018). "Semantically Equivalent Adversarial Rules for Debugging NLP Models." *ACL*.
4. Ye, M., et al. (2020). "SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions." *ACL*.
5. Wallace, E., et al. (2019). "Universal Adversarial Triggers for Attacking and Analyzing NLP." *EMNLP*.
