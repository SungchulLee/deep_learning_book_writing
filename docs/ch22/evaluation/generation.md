# Generation Quality

Evaluating the quality and diversity of samples generated by trained VAEs.

---

## Learning Objectives

By the end of this section, you will be able to:

- Compute FID and IS scores for VAE-generated samples
- Evaluate sample diversity and mode coverage
- Compare generation quality across VAE variants
- Understand the precision-recall framework for generative models

---

## Generation Pipeline

To generate samples from a trained VAE:

```python
def generate_samples(model, num_samples, device):
    """Generate samples by sampling from the prior and decoding."""
    model.eval()
    with torch.no_grad():
        z = torch.randn(num_samples, model.latent_dim).to(device)
        samples = model.decode(z)
    return samples
```

---

## Fréchet Inception Distance (FID)

### Definition

FID measures the distance between the distribution of generated and real images in the feature space of a pretrained Inception network:

$$\text{FID} = \|\mu_r - \mu_g\|^2 + \text{tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)$$

where $(\mu_r, \Sigma_r)$ and $(\mu_g, \Sigma_g)$ are the mean and covariance of Inception features for real and generated images, respectively.

**Lower FID = Better quality.** FID captures both fidelity (do generated samples look realistic?) and diversity (do they cover the full distribution?).

### Computation

```python
# Using torchmetrics
from torchmetrics.image.fid import FrechetInceptionDistance

fid = FrechetInceptionDistance(feature=2048)
fid.update(real_images, real=True)
fid.update(generated_images, real=False)
fid_score = fid.compute()
```

---

## Inception Score (IS)

The Inception Score evaluates both quality (high confidence predictions) and diversity (uniform class distribution):

$$\text{IS} = \exp\left(\mathbb{E}_x[D_{KL}(p(y|x) \| p(y))]\right)$$

**Higher IS = Better.** However, IS has known limitations: it doesn't compare against real data and can be gamed by generating one perfect sample per class.

---

## Precision and Recall

### Framework

Precision measures what fraction of generated samples fall within the real data manifold (fidelity), while recall measures what fraction of the real data manifold is covered by generated samples (diversity).

VAEs typically have **high recall** (good mode coverage) but **moderate precision** (some blurry samples fall outside the real manifold). GANs have the opposite profile: high precision but potentially low recall due to mode collapse.

---

## Visual Evaluation

### Random Samples Grid

Generate a grid of samples to assess overall quality:

```python
def visualize_samples(model, device, grid_size=10):
    """Generate and display a grid of samples."""
    samples = generate_samples(model, grid_size**2, device)
    
    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))
    for i in range(grid_size):
        for j in range(grid_size):
            idx = i * grid_size + j
            axes[i, j].imshow(samples[idx].cpu().reshape(28, 28), cmap='gray')
            axes[i, j].axis('off')
    plt.tight_layout()
```

### Interpolation Quality

Smooth interpolation between latent codes indicates a well-structured latent space:

```python
def interpolate(model, z1, z2, steps=10, device='cpu'):
    """Linear interpolation in latent space."""
    model.eval()
    with torch.no_grad():
        interps = []
        for t in torch.linspace(0, 1, steps):
            z = (1 - t) * z1 + t * z2
            interps.append(model.decode(z.to(device)).cpu())
    return torch.stack(interps)
```

---

## Summary

| Metric | Measures | VAE Typical Performance |
|--------|----------|------------------------|
| **FID** | Fidelity + diversity | Moderate (higher than GAN) |
| **IS** | Quality + diversity | Moderate |
| **Precision** | Sample fidelity | Moderate (blurriness hurts) |
| **Recall** | Mode coverage | High (good coverage) |

---

## Exercises

### Exercise 1

Generate 10,000 samples from a trained VAE and compute FID against the MNIST test set.

### Exercise 2

Compare FID scores across VAE variants: standard VAE, β-VAE (β=4), and VQ-VAE.

---

## What's Next

The next section covers [Latent Space Quality](latent_space.md) evaluation.
