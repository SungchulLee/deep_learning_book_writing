# Example 4: Advanced Transfer Learning Techniques

## ğŸ¯ Learning Objectives

By completing this example, you will learn:
- Advanced learning rate scheduling strategies
- Mixed precision training for faster computation
- Gradient accumulation for larger effective batch sizes
- Working with different pre-trained architectures
- Model ensembling techniques
- Advanced optimization tricks

## ğŸ“‹ Overview

This example demonstrates **state-of-the-art transfer learning techniques** used in research and production environments.

**Advanced Techniques Covered:**
1. Cosine Annealing learning rate schedule
2. Mixed precision training (FP16)
3. Gradient accumulation
4. Multiple pre-trained architectures
5. Model ensembling
6. Gradual layer unfreezing
7. Test-time augmentation

## ğŸš€ Why These Techniques Matter

### Mixed Precision Training
- âš¡ 2-3x faster training
- ğŸ’¾ Reduced memory usage
- ğŸ¯ Minimal accuracy loss

### Gradient Accumulation
- ğŸ“ˆ Simulate larger batch sizes without memory issues
- ğŸ”„ Better gradient estimates
- ğŸ’ª More stable training

### Advanced Schedulers
- ğŸ¢ Better convergence properties
- ğŸ¯ Escape local minima
- ğŸ“Š Improved final performance

### Model Ensembling
- ğŸ† Best possible accuracy
- ğŸ›¡ï¸ More robust predictions
- âš–ï¸ Combines different model strengths

## ğŸ’» Running the Code

```bash
python advanced_techniques.py
```

**Expected Runtime:** 15-20 minutes on GPU

**Note:** Some features require a CUDA-capable GPU:
- Mixed precision training (automatic fallback to FP32 on CPU)

## ğŸ“Š Expected Results

You should see:
- Training accuracy: ~92-97%
- Test accuracy: ~88-93%
- Significant speedup with mixed precision
- Best results when using ensemble

## ğŸ”§ Configuration Options

The script includes several advanced options you can toggle:

```python
USE_MIXED_PRECISION = True      # Enable FP16 training
GRADIENT_ACCUMULATION_STEPS = 4  # Simulate 4x larger batch
USE_COSINE_SCHEDULE = True       # Use cosine annealing
GRADUAL_UNFREEZING = True        # Unfreeze layers progressively
```

## ğŸ“ Technique Details

### 1. Cosine Annealing Schedule
```
Learning Rate
    |     ___
    |    /   \___
    |   /        \___
    |  /             \___
    | /                  \___
    |/________________________\
                Epochs
```
Benefits:
- Smooth learning rate decay
- Multiple restarts help escape local minima
- Often better than step decay

### 2. Mixed Precision Training
Uses FP16 (16-bit) for most operations:
- Forward pass: FP16
- Backward pass: FP16
- Weight updates: FP32 (prevents underflow)

### 3. Gradient Accumulation
```
Normal: Update every batch
Accumulated: Update every N batches

Effective batch size = batch_size Ã— accumulation_steps
```

### 4. Gradual Unfreezing
```
Epoch 1-5:   Only train classifier
Epoch 6-10:  + Unfreeze layer4
Epoch 11-15: + Unfreeze layer3
Epoch 16-20: + Unfreeze layer2
```

## ğŸ—ï¸ Architecture Comparison

The script demonstrates multiple architectures:

| Model | Parameters | Speed | Accuracy | Best For |
|-------|-----------|-------|----------|----------|
| ResNet18 | 11M | Fast | Good | Quick prototyping |
| ResNet50 | 25M | Medium | Better | Balanced performance |
| EfficientNet-B0 | 5M | Fast | Best | Production deployment |

## ğŸ¯ When to Use These Techniques

### Use Mixed Precision When:
- You have a modern GPU (RTX, V100, A100)
- Training time is a bottleneck
- Memory is limited

### Use Gradient Accumulation When:
- Batch size is limited by memory
- You need more stable gradients
- Training on smaller GPUs

### Use Ensemble When:
- You need maximum accuracy
- Inference time is not critical
- You have computational resources

### Use Gradual Unfreezing When:
- Transfer domain is very different
- You have limited data
- You want to prevent catastrophic forgetting

## ğŸš€ Next Steps

After mastering these techniques:
- Apply to your own challenging datasets
- Combine multiple techniques for best results
- Experiment with different architectures
- Read research papers on latest techniques

## ğŸ“š Advanced Topics (Beyond This Tutorial)

To go even further, explore:
- Knowledge distillation
- Neural architecture search
- Self-supervised pre-training
- Meta-learning approaches
- Domain adaptation techniques

## ğŸ’¡ Pro Tips

1. **Start Simple**: Don't use all techniques at once
2. **Profile First**: Identify bottlenecks before optimizing
3. **Monitor Everything**: Track loss, accuracy, learning rate
4. **Compare Fairly**: Use same data splits for comparisons
5. **Read Papers**: Stay updated with latest research

## âš ï¸ Important Notes

- Mixed precision requires CUDA compute capability â‰¥ 7.0
- Gradient accumulation increases training time per epoch
- Ensemble requires multiple model trainings
- Some techniques may not help for all datasets

## ğŸ¤” Troubleshooting

**NaN Loss with Mixed Precision:**
- Enable loss scaling (automatic in modern PyTorch)
- Check for numerical instabilities
- Reduce learning rate

**No Speed Improvement:**
- Ensure you're using GPU
- Check CUDA compatibility
- Verify batch size is large enough

**Ensemble Not Helping:**
- Train more diverse models
- Use different architectures
- Try different random seeds
