# Example 2: Fine-Tuning a Pre-trained Model

## ğŸ¯ Learning Objectives

By completing this example, you will learn:
- The difference between feature extraction and fine-tuning
- How to selectively unfreeze layers for training
- Using different learning rates for different parts of the network
- Implementing early stopping to prevent overfitting
- Best practices for fine-tuning

## ğŸ“‹ Overview

This example builds on Example 1 by introducing **fine-tuning**.

**The Difference:**
- **Feature Extraction (Example 1):** Only train the final layer
- **Fine-Tuning (Example 2):** Train multiple layers with different learning rates

**The Strategy:**
1. Start with a pre-trained model (ResNet18)
2. Replace the final layer
3. Initially freeze early layers, unfreeze later layers
4. Use different learning rates: smaller for pre-trained layers, larger for new layers
5. Optionally fine-tune more layers as training progresses

## ğŸ” What's Happening?

```
Pre-trained ResNet18 (ImageNet)
        â†“
[Early Conv Layers] â† FROZEN (generic features)
        â†“
[Middle Conv Layers] â† FINE-TUNING (adapt features, small LR)
        â†“
[Late Conv Layers] â† FINE-TUNING (task-specific features, small LR)
        â†“
[New Classifier] â† TRAINING (large LR)
        â†“
[10 Classes Output]
```

## ğŸ¤” When to Use Fine-Tuning?

**Use Fine-Tuning When:**
- Your dataset is different from ImageNet
- You have sufficient training data (thousands of samples)
- You want to achieve the best possible accuracy
- You have computational resources for longer training

**Use Feature Extraction When:**
- Your dataset is similar to ImageNet
- You have limited data (hundreds of samples)
- You need quick results
- Computational resources are limited

## ğŸ’» Running the Code

```bash
python fine_tuning.py
```

**Expected Runtime:** 10-15 minutes on GPU, 30-45 minutes on CPU

## ğŸ“Š Expected Results

Compared to Example 1, you should see:
- Training accuracy: ~90-95%
- Test accuracy: ~85-90%
- Better performance but longer training time
- The model takes more epochs to converge

## ğŸ”§ Hyperparameters

Default settings:
- Batch size: 32
- Learning rate (pre-trained layers): 0.0001
- Learning rate (new layer): 0.001
- Optimizer: Adam
- Epochs: 15
- Dataset: CIFAR-10

## ğŸ“ Key Concepts

### 1. Discriminative Learning Rates
Different parts of the network learn different things:
- Early layers: Generic features (edges, colors) - use small LR
- Late layers: Task-specific features - use medium LR
- New classifier: Random initialization - use large LR

### 2. Gradual Unfreezing
Start by training only the classifier, then gradually unfreeze more layers. This prevents destroying pre-trained weights early in training.

### 3. Early Stopping
Stop training when validation performance stops improving to prevent overfitting.

## ğŸš€ Next Steps

After understanding this example:
- Experiment with different unfreezing strategies
- Try gradual unfreezing (unfreeze more layers over time)
- Compare results with Example 1
- Move on to Example 3 to work with custom datasets!

## ğŸ¤” Questions to Think About

1. Why do we use a smaller learning rate for pre-trained layers?
2. What happens if we use the same learning rate for all layers?
3. When might fine-tuning perform worse than feature extraction?

Experiment with the code to find the answers!
