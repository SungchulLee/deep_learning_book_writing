# Maximum Likelihood Derivation for Linear Regression

## Overview

Maximum Likelihood Estimation (MLE) provides a principled statistical framework for deriving the optimal parameters in linear regression. This derivation connects the probabilistic interpretation of linear regression to the practical loss functions used in training.

## Probabilistic Model

### The Generative Story

In the probabilistic view of linear regression, we assume data is generated by:

1. The true relationship is linear: $\mu_i = \mathbf{w}^T\mathbf{x}_i + b$
2. Observations are corrupted by Gaussian noise: $y_i = \mu_i + \epsilon_i$
3. Noise is i.i.d.: $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$

This implies:

$$y_i | \mathbf{x}_i \sim \mathcal{N}(\mathbf{w}^T\mathbf{x}_i + b, \sigma^2)$$

### Probability Density Function

The conditional probability density of a single observation is:

$$p(y_i | \mathbf{x}_i; \mathbf{w}, b, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2}{2\sigma^2}\right)$$

## Likelihood Function

### Definition

For $n$ independent observations $\mathcal{D} = \{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$, the likelihood is the joint probability:

$$\mathcal{L}(\mathbf{w}, b, \sigma^2 | \mathcal{D}) = \prod_{i=1}^{n} p(y_i | \mathbf{x}_i; \mathbf{w}, b, \sigma^2)$$

### Expanded Form

$$\mathcal{L}(\mathbf{w}, b, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2}{2\sigma^2}\right)$$

$$= \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2\right)$$

## Log-Likelihood

### Taking the Logarithm

Products become sums under the log transform, simplifying optimization:

$$\ell(\mathbf{w}, b, \sigma^2) = \log \mathcal{L}(\mathbf{w}, b, \sigma^2)$$

$$= -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2$$

### Simplification

The terms not involving parameters $\mathbf{w}$ and $b$ are constants with respect to optimization:

$$\ell(\mathbf{w}, b) \propto -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2$$

## Maximum Likelihood Estimation

### Optimization Objective

Maximizing log-likelihood is equivalent to minimizing the negative log-likelihood:

$$\hat{\mathbf{w}}, \hat{b} = \arg\max_{\mathbf{w}, b} \ell(\mathbf{w}, b) = \arg\min_{\mathbf{w}, b} \sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2$$

This is the **Sum of Squared Errors (SSE)**!

### Mean Squared Error Connection

Dividing by $n$ gives the Mean Squared Error:

$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**Key Insight**: Minimizing MSE is equivalent to maximizing the likelihood under Gaussian noise assumptions.

## Deriving the Gradient

### Gradient with Respect to Weights

For the loss function $L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x}_i - b)^2$:

$$\frac{\partial L}{\partial w_j} = \frac{2}{n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x}_i - b)(-x_{ij})$$

$$= -\frac{2}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)x_{ij}$$

In vector form:

$$\nabla_{\mathbf{w}} L = -\frac{2}{n}\mathbf{X}^T(\mathbf{y} - \hat{\mathbf{y}}) = \frac{2}{n}\mathbf{X}^T(\hat{\mathbf{y}} - \mathbf{y})$$

### Gradient with Respect to Bias

$$\frac{\partial L}{\partial b} = -\frac{2}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i) = \frac{2}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)$$

## PyTorch Implementation

### Manual Gradient Computation

```python
import torch
import numpy as np

def compute_mse_loss(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:
    """
    Compute Mean Squared Error loss
    
    MSE = (1/n) * sum((y_true - y_pred)^2)
    
    This is the negative log-likelihood (up to constants) under
    Gaussian noise assumption.
    """
    n = y_true.shape[0]
    return torch.sum((y_true - y_pred) ** 2) / n

def compute_gradients_manual(
    X: torch.Tensor, 
    y_true: torch.Tensor, 
    y_pred: torch.Tensor
) -> tuple:
    """
    Manually compute gradients of MSE w.r.t. w and b
    
    ∂MSE/∂w = (2/n) * X^T @ (y_pred - y_true)
    ∂MSE/∂b = (2/n) * sum(y_pred - y_true)
    """
    n = X.shape[0]
    error = y_pred - y_true  # (n, 1)
    
    # Gradient w.r.t. weights
    grad_w = (2.0 / n) * (X.T @ error)  # (d, 1)
    
    # Gradient w.r.t. bias
    grad_b = (2.0 / n) * torch.sum(error)  # scalar
    
    return grad_w, grad_b

# Example
torch.manual_seed(42)
n_samples, n_features = 100, 5
X = torch.randn(n_samples, n_features)
y_true = torch.randn(n_samples, 1)

# Initialize parameters
w = torch.zeros(n_features, 1)
b = torch.tensor([0.0])

# Forward pass
y_pred = X @ w + b

# Compute loss and gradients
loss = compute_mse_loss(y_true, y_pred)
grad_w, grad_b = compute_gradients_manual(X, y_true, y_pred)

print(f"Loss: {loss.item():.4f}")
print(f"Gradient w shape: {grad_w.shape}")
print(f"Gradient b: {grad_b.item():.4f}")
```

### Verifying Against Autograd

```python
def verify_gradients():
    """Verify manual gradients match PyTorch autograd"""
    torch.manual_seed(42)
    
    n_samples, n_features = 50, 3
    X = torch.randn(n_samples, n_features)
    y_true = torch.randn(n_samples, 1)
    
    # Parameters with gradient tracking
    w = torch.zeros(n_features, 1, requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    
    # Forward pass
    y_pred = X @ w + b
    
    # Compute loss
    loss = torch.mean((y_true - y_pred) ** 2)
    
    # Autograd gradients
    loss.backward()
    
    # Manual gradients
    with torch.no_grad():
        y_pred_manual = X @ w + b
        grad_w_manual, grad_b_manual = compute_gradients_manual(X, y_true, y_pred_manual)
    
    # Compare
    print("Gradient Verification:")
    print(f"  w gradient match: {torch.allclose(w.grad, grad_w_manual)}")
    print(f"  b gradient match: {torch.allclose(b.grad.squeeze(), grad_b_manual)}")
    print(f"  Max difference (w): {torch.max(torch.abs(w.grad - grad_w_manual)).item():.2e}")
    print(f"  Max difference (b): {torch.abs(b.grad.squeeze() - grad_b_manual).item():.2e}")

verify_gradients()
```

## Variance Estimation

### MLE for Noise Variance

Setting $\frac{\partial \ell}{\partial \sigma^2} = 0$:

$$\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

This is a **biased** estimator. The unbiased estimator is:

$$\hat{\sigma}^2_{unbiased} = \frac{1}{n-d-1}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

where $d$ is the number of features.

```python
def estimate_noise_variance(
    y_true: torch.Tensor, 
    y_pred: torch.Tensor, 
    n_features: int,
    unbiased: bool = True
) -> float:
    """
    Estimate noise variance from residuals
    
    Args:
        y_true: True target values
        y_pred: Model predictions
        n_features: Number of features in the model
        unbiased: If True, use n-d-1 denominator
    """
    n = y_true.shape[0]
    residuals = y_true - y_pred
    ss_residuals = torch.sum(residuals ** 2)
    
    if unbiased:
        # Degrees of freedom: n - d - 1 (for d features + 1 bias)
        dof = n - n_features - 1
    else:
        dof = n
    
    return (ss_residuals / dof).item()
```

## Complete MLE Training Loop

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

def mle_training_loop(
    X: torch.Tensor,
    y: torch.Tensor,
    n_epochs: int = 100,
    learning_rate: float = 0.01
) -> dict:
    """
    Train linear regression via gradient descent on negative log-likelihood
    
    The NLL under Gaussian noise is proportional to MSE.
    """
    n_samples, n_features = X.shape
    
    # Initialize parameters
    w = torch.zeros(n_features, 1, requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    
    # Storage for history
    history = {'loss': [], 'w': [], 'b': []}
    
    for epoch in range(n_epochs):
        # Forward pass
        y_pred = X @ w + b
        
        # Compute negative log-likelihood (proportional to MSE)
        # NLL = (n/2)*log(2*pi*sigma^2) + (1/(2*sigma^2)) * sum((y - y_pred)^2)
        # For optimization, this simplifies to MSE
        loss = torch.mean((y - y_pred) ** 2)
        
        # Backward pass
        loss.backward()
        
        # Gradient descent update
        with torch.no_grad():
            w -= learning_rate * w.grad
            b -= learning_rate * b.grad
        
        # Zero gradients for next iteration
        w.grad.zero_()
        b.grad.zero_()
        
        # Store history
        history['loss'].append(loss.item())
        history['w'].append(w.clone().detach())
        history['b'].append(b.item())
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1:3d}: Loss = {loss.item():.6f}")
    
    return {'w': w.detach(), 'b': b.item(), 'history': history}

# Example with synthetic data
torch.manual_seed(42)

# True parameters
true_w = torch.tensor([[2.0], [-1.5], [0.5]])
true_b = 1.0
sigma = 0.5

# Generate data
n_samples = 200
X = torch.randn(n_samples, 3)
y = X @ true_w + true_b + sigma * torch.randn(n_samples, 1)

# Train
print("Training Linear Regression via MLE:")
print("=" * 50)
result = mle_training_loop(X, y, n_epochs=100, learning_rate=0.1)

print("\nResults:")
print(f"True w: {true_w.squeeze().tolist()}")
print(f"Learned w: {result['w'].squeeze().tolist()}")
print(f"True b: {true_b}")
print(f"Learned b: {result['b']:.4f}")
```

## Visualizing the MLE Objective

```python
def visualize_mle_landscape():
    """Visualize the likelihood landscape for 1D linear regression"""
    # Simple 1D problem
    torch.manual_seed(42)
    n = 50
    X = torch.linspace(-3, 3, n).reshape(-1, 1)
    true_w, true_b = 2.0, 1.0
    y = true_w * X + true_b + 0.5 * torch.randn(n, 1)
    
    # Create grid for visualization
    w_range = torch.linspace(-1, 5, 100)
    b_range = torch.linspace(-2, 4, 100)
    W, B = torch.meshgrid(w_range, b_range, indexing='ij')
    
    # Compute negative log-likelihood (MSE) at each point
    NLL = torch.zeros_like(W)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            y_pred = W[i, j] * X + B[i, j]
            NLL[i, j] = torch.mean((y - y_pred) ** 2)
    
    # Plot
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Contour plot
    ax1 = axes[0]
    contour = ax1.contour(W.numpy(), B.numpy(), NLL.numpy(), levels=30)
    ax1.clabel(contour, inline=True, fontsize=8)
    ax1.scatter([true_w], [true_b], color='red', s=100, marker='*', label='True')
    ax1.set_xlabel('w (weight)')
    ax1.set_ylabel('b (bias)')
    ax1.set_title('Negative Log-Likelihood Contours')
    ax1.legend()
    
    # 3D surface
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')
    ax2.plot_surface(W.numpy(), B.numpy(), NLL.numpy(), cmap='viridis', alpha=0.8)
    ax2.set_xlabel('w')
    ax2.set_ylabel('b')
    ax2.set_zlabel('NLL (MSE)')
    ax2.set_title('Likelihood Surface')
    
    plt.tight_layout()
    return fig

# Note: Run visualize_mle_landscape() to see the likelihood landscape
```

## Key Mathematical Results

### Summary of Derivation

| Step | Expression | Interpretation |
|------|------------|----------------|
| Model | $y_i = \mathbf{w}^T\mathbf{x}_i + b + \epsilon_i$ | Linear function + noise |
| Noise | $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ | Gaussian assumption |
| Likelihood | $\prod_i p(y_i \| \mathbf{x}_i)$ | Joint probability |
| Log-likelihood | $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\text{SSE}$ | Log of likelihood |
| MLE objective | $\min \sum_i (y_i - \hat{y}_i)^2$ | Minimize SSE |
| Equivalent to | $\min \text{MSE}$ | Mean Squared Error |

### Why Gaussian Noise?

1. **Central Limit Theorem**: Sum of many small independent effects is approximately Gaussian
2. **Maximum Entropy**: Gaussian is the maximum entropy distribution for given mean and variance
3. **Analytical Tractability**: Leads to closed-form solutions
4. **Historical Precedent**: Gauss originally derived least squares assuming this model

## Summary

The MLE derivation reveals that:

1. **MSE emerges naturally** from Gaussian noise assumptions
2. **Log transform** converts products to sums, simplifying optimization
3. **Gradient descent** on MSE is equivalent to maximizing likelihood
4. **Variance estimation** provides confidence in predictions
5. **Probabilistic view** enables uncertainty quantification

## References

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*, Chapter 3
- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*, Chapter 7
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*, Chapter 3
