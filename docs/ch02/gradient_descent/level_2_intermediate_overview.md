# Level 2: Intermediate - Improving Gradient Descent ğŸ’ª

Welcome to Level 2! Here you'll learn techniques to make gradient descent faster and more effective.

## Learning Objectives

By completing this level, you will:
- âœ… Understand batch, mini-batch, and stochastic gradient descent
- âœ… Implement momentum for faster convergence
- âœ… Use learning rate schedules
- âœ… Apply techniques to non-linear problems

## Time Estimate
**3-4 hours** (including experimentation)

## Prerequisites
- Completed Level 1
- Understanding of basic gradient descent
- Comfortable with PyTorch DataLoader

## Examples in This Level

### 01_batch_vs_minibatch_sgd.py â­â­
Compare three variants of gradient descent and understand their trade-offs.

**Run it:**
```bash
python 01_batch_vs_minibatch_sgd.py
```

### 02_momentum_optimizer.py â­â­
Learn how momentum accelerates convergence and escapes local minima.

### 03_learning_rate_schedules.py â­â­
Implement learning rate decay strategies for better final convergence.

### 04_polynomial_regression.py â­â­
Apply gradient descent to fit non-linear functions.

## Key Concepts

- **Mini-batch GD**: Sweet spot between batch and stochastic
- **Momentum**: Accelerates in consistent directions
- **Learning rate scheduling**: Start large, decay over time
- **Feature engineering**: Transform inputs for better fit

## Next Steps

After mastering these techniques, move to:
**Level 3: Advanced** - Learn Adam, RMSprop, and advanced optimization

---

Good luck! ğŸš€
