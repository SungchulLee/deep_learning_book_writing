# BEiT: BERT Pre-Training of Image Transformers

BEiT adapts the masked language modeling objective from BERT to vision. Instead of predicting raw pixels, BEiT predicts discrete visual tokens generated by a pretrained discrete VAE tokenizer.

## Architecture

```
Image → Patch Embedding → [Randomly mask ~40% of patches]
                              │
                    ┌─────────┴──────────┐
                    │                    │
              Visible patches       [MASK] tokens
                    │                    │
                    └─────────┬──────────┘
                              │
                    Vision Transformer Encoder
                              │
                    Predict visual tokens for masked positions
```

## Key Innovation: Visual Tokenizer

BEiT uses a discrete VAE (dVAE) to create a visual vocabulary. Each image patch is mapped to one of ~8192 discrete tokens, and the model learns to predict these tokens for masked patches.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class BEiTPretraining(nn.Module):
    """BEiT masked image modeling with discrete visual tokens."""
    
    def __init__(self, vit_encoder, vocab_size=8192, mask_ratio=0.4):
        super().__init__()
        self.encoder = vit_encoder
        self.mask_ratio = mask_ratio
        hidden_dim = 768  # ViT-B default
        self.head = nn.Linear(hidden_dim, vocab_size)
    
    def random_masking(self, x, mask_ratio):
        B, N, D = x.shape
        num_mask = int(N * mask_ratio)
        noise = torch.rand(B, N, device=x.device)
        ids_shuffle = torch.argsort(noise, dim=1)
        mask = torch.zeros(B, N, device=x.device)
        mask.scatter_(1, ids_shuffle[:, :num_mask], 1)
        return mask.bool()
    
    def forward(self, patches, visual_tokens):
        """
        Args:
            patches: Image patches (B, N, D)
            visual_tokens: Target discrete tokens from dVAE (B, N)
        """
        mask = self.random_masking(patches, self.mask_ratio)
        encoded = self.encoder(patches, mask=mask)
        
        masked_output = encoded[mask]
        logits = self.head(masked_output)
        target_tokens = visual_tokens[mask]
        loss = F.cross_entropy(logits, target_tokens)
        return loss
```

## BEiT vs MAE

| Aspect | BEiT | MAE |
|--------|------|-----|
| Target | Discrete visual tokens | Raw pixels |
| Tokenizer | Requires pretrained dVAE | None needed |
| Mask ratio | ~40% | ~75% |
| Decoder | Linear head | Lightweight decoder |
| Complexity | Higher (needs tokenizer) | Simpler |

## References

1. Bao, H., et al. (2022). "BEiT: BERT Pre-Training of Image Transformers." *ICLR*.
2. Peng, Z., et al. (2022). "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers." arXiv.
