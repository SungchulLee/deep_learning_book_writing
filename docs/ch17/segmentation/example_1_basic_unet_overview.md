# Example 1: Basic U-Net Semantic Segmentation

## ğŸ¯ Learning Objectives

By completing this example, you will learn:
- What semantic segmentation is and how it differs from classification
- The U-Net architecture (encoder-decoder with skip connections)
- How to implement U-Net from scratch in PyTorch
- Pixel-wise loss functions
- IoU (Intersection over Union) metric
- Data augmentation for segmentation tasks
- Binary segmentation fundamentals

## ğŸ“‹ Overview

This example introduces semantic segmentation using the classic **U-Net architecture**. We'll start with a simple binary segmentation task (2 classes: background and foreground) on a synthetic dataset.

**Why U-Net?**
- Simple and intuitive architecture
- Works well with limited data
- Widely used in medical imaging
- Foundation for many modern architectures
- Skip connections preserve spatial information

## ğŸ—ï¸ U-Net Architecture Explained

```
Input (256Ã—256Ã—3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENCODER (Downsampling Path)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Conv    â”‚â†’ â”‚ Conv    â”‚â†’ â”‚ MaxPool â”‚  â”‚
â”‚  â”‚ 64      â”‚  â”‚ 64      â”‚  â”‚ Ã·2      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚ Skip Connection 1        â”‚        â”‚
â”‚       â”‚                     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”‚
â”‚       â”‚                     â”‚ Conv    â”‚  â”‚
â”‚       â”‚                     â”‚ 128     â”‚  â”‚
â”‚       â”‚                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚ Skip Connection 2        â”‚        â”‚
â”‚       â”‚                     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”‚
â”‚       â”‚                     â”‚ Conv    â”‚  â”‚
â”‚       â”‚                     â”‚ 256     â”‚  â”‚
â”‚       â”‚                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚                          â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       â”‚     BOTTLENECK           â”‚        â”‚
â”‚       â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚        â”‚
â”‚       â”‚     â”‚ Conv   â”‚           â”‚        â”‚
â”‚       â”‚     â”‚ 512    â”‚           â”‚        â”‚
â”‚       â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜           â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚          â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECODER (Upsampling Path)       â”‚        â”‚
â”‚       â”‚     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”          â”‚        â”‚
â”‚       â”‚     â”‚ Upsampleâ”‚          â”‚        â”‚
â”‚       â”‚     â”‚ Ã—2      â”‚          â”‚        â”‚
â”‚       â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚        â”‚
â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚        â”‚
â”‚       â””â”€â†’â”‚ Concatenate    â”‚      â”‚        â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚        â”‚
â”‚          â”‚ Conv 256       â”‚      â”‚        â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚
â”‚                  â‹®               â”‚        â”‚
â”‚          [Repeat upsampling]     â”‚        â”‚
â”‚                  â”‚               â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        
                   â†“
        Output (256Ã—256Ã—2)
        [Background, Foreground]
```

**Key Components:**
1. **Encoder:** Captures context, reduces spatial size
2. **Bottleneck:** Highest level features
3. **Decoder:** Recovers spatial resolution
4. **Skip Connections:** Preserve fine-grained details

## ğŸ¨ Binary Segmentation Task

We'll segment simple shapes from backgrounds:

```
Input Image:          Ground Truth Mask:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â—‹â—‹â—‹â—‹â—‹     â”‚      â”‚   11111     â”‚
â”‚  â—‹     â—‹    â”‚  â†’   â”‚  1     1    â”‚
â”‚   â—‹â—‹â—‹â—‹â—‹     â”‚      â”‚   11111     â”‚
â”‚             â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” What's Happening?

1. **Input:** RGB image (3 channels)
2. **Encoder:** Extracts features, reduces size (256â†’128â†’64â†’32)
3. **Bottleneck:** Processes deepest features
4. **Decoder:** Upsamples back to original size (32â†’64â†’128â†’256)
5. **Skip Connections:** Copy features from encoder to decoder at each level
6. **Output:** 2-channel prediction (background, foreground)

## ğŸ’» Running the Code

```bash
python basic_unet_segmentation.py
```

**Expected Runtime:** 5-10 minutes on GPU, 15-25 minutes on CPU

## ğŸ“Š Expected Results

You should see:
- Training IoU: ~85-95%
- Validation IoU: ~80-90%
- Clear visualization of predictions
- Model learns to segment shapes accurately

## ğŸ”§ Hyperparameters

Default settings:
- Input size: 256Ã—256
- Batch size: 8
- Learning rate: 0.001
- Optimizer: Adam
- Epochs: 20
- Loss: Binary Cross-Entropy
- Dataset: Synthetic shapes

## ğŸ“ Evaluation Metrics

### IoU (Intersection over Union)
The primary metric for segmentation:
```
IoU = Area of Overlap / Area of Union
    = True Positives / (True Positives + False Positives + False Negatives)
```

**Example:**
```
Prediction:    Ground Truth:   Intersection:   Union:
â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆ â”‚       â”‚ â–ˆâ–ˆâ–ˆ â”‚         â”‚ â–ˆâ–ˆ  â”‚        â”‚ â–ˆâ–ˆâ–ˆ â”‚
â”‚ â–ˆâ–ˆâ–ˆ â”‚   &   â”‚ â–ˆâ–ˆ  â”‚    =    â”‚ â–ˆâ–ˆ  â”‚   |    â”‚ â–ˆâ–ˆâ–ˆ â”‚
â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”˜

IoU = 4 / 6 = 0.67
```

**IoU Interpretation:**
- 0.9-1.0: Excellent
- 0.7-0.9: Good
- 0.5-0.7: Acceptable
- <0.5: Poor

### Pixel Accuracy
```
Accuracy = Correct Pixels / Total Pixels
```
Note: Can be misleading if classes are imbalanced!

## ğŸ“ Key Takeaways

1. **Encoder-Decoder Structure**: Downsampling captures context, upsampling recovers resolution
2. **Skip Connections**: Critical for preserving spatial details
3. **Pixel-wise Loss**: Each pixel contributes to the loss
4. **Data Augmentation**: Must apply same transform to image AND mask
5. **IoU Metric**: Better than accuracy for segmentation

## ğŸš€ Next Steps

After understanding this example:
- Visualize feature maps at different layers
- Try different loss functions (Dice loss)
- Experiment with different encoder depths
- Add more augmentation
- Move on to Example 2 for pre-trained encoders!

## ğŸ¤” Questions to Think About

1. Why do we use skip connections?
2. What happens if we remove skip connections?
3. Why is pixel accuracy not enough?
4. How does segmentation differ from classification?

Experiment with the code to find the answers!

## ğŸ’¡ Extension Ideas

- Try 3+ classes (multi-class segmentation)
- Implement Dice loss instead of BCE
- Add more complex shapes
- Visualize what each layer learns
- Try different decoder architectures
