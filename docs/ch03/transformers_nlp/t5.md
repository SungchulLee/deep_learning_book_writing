# T5: Text-to-Text Transfer Transformer

## Overview

T5 (Raffel et al., 2020) is an **encoder-decoder** Transformer that frames all NLP tasks as text-to-text problems. Given text input, produce text output—whether the task is classification, translation, summarization, or question answering.

## Architecture

T5 uses the **full encoder-decoder** Transformer:

$$\text{Input text} \xrightarrow{\text{Encoder}} \mathbf{M} \xrightarrow{\text{Decoder}} \text{Output text}$$

| Model | Layers | Hidden | Heads | Parameters |
|-------|--------|--------|-------|------------|
| T5-small | 6 | 512 | 8 | 60M |
| T5-base | 12 | 768 | 12 | 220M |
| T5-large | 24 | 1024 | 16 | 770M |
| T5-3B | 24 | 1024 | 32 | 3B |
| T5-11B | 24 | 1024 | 128 | 11B |

## Text-to-Text Framework

**Every task becomes:** Input text → T5 → Output text

### Classification
```
Input:  "sst2 sentence: This movie is great."
Output: "positive"
```

### Translation
```
Input:  "translate English to German: Hello, how are you?"
Output: "Hallo, wie geht es dir?"
```

### Summarization
```
Input:  "summarize: The quick brown fox jumped over the lazy dog..."
Output: "A fox jumped over a dog."
```

### Question Answering
```
Input:  "question: What is the capital of France? context: Paris is the capital..."
Output: "Paris"
```

The task is specified as a **prefix** in the input.

## Pre-training Objective

### Span Corruption (Denoising)

Randomly corrupt spans of text and train to reconstruct:

**Original**: "The quick brown fox jumps over the lazy dog"

**Corrupted input**: "The \<X\> brown \<Y\> over the lazy dog"

**Target output**: "\<X\> quick \<Y\> fox jumps"

- Spans of varying length are replaced with sentinel tokens
- Model learns to predict the missing spans
- More efficient than token-level masking (BERT)

### Why Spans?

| Objective | Corruption | Pro | Con |
|-----------|------------|-----|-----|
| Token masking (BERT) | 15% random tokens | Simple | Short-range dependencies |
| Span corruption (T5) | ~15% of tokens in spans | Longer context | More complex |

Span corruption encourages learning longer-range dependencies.

## Encoder-Decoder Attention

Three types of attention in T5:

1. **Encoder self-attention**: Bidirectional, no mask
2. **Decoder self-attention**: Causal mask
3. **Decoder cross-attention**: Attend to encoder output

```
Encoder:                    Decoder:
┌──────────────────┐        ┌──────────────────┐
│ Self-Attention   │   ┌──→ │ Masked Self-Attn │
│ (bidirectional)  │   │    ├──────────────────┤
├──────────────────┤   │    │ Cross-Attention  │◄─┐
│      FFN         │   │    ├──────────────────┤  │
└────────┬─────────┘   │    │      FFN         │  │
         │             │    └──────────────────┘  │
         └─────────────┴──────────────────────────┘
         Encoder output (Memory)
```

## Relative Position Encodings

T5 uses **learned relative position biases** instead of absolute encodings:

$$A_{ij} = \frac{\mathbf{q}_i^T \mathbf{k}_j}{\sqrt{d_k}} + b_{i-j}$$

where $b_{i-j}$ is a learned bias depending on relative distance.

Benefits:
- Better extrapolation to longer sequences
- Position-independent representations

## Training Details

### C4 Dataset

"Colossal Clean Crawled Corpus" - 750GB of cleaned web text

Filtering:
- English only
- Removed pages with bad words
- Deduplicated
- Removed boilerplate

### Multi-task Training

T5 is trained on a mixture of:
- Unsupervised span corruption (C4)
- Supervised tasks (translation, summarization, QA, etc.)

Tasks are sampled proportionally to dataset size with temperature mixing.

## PyTorch Usage

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load model
tokenizer = T5Tokenizer.from_pretrained('t5-base')
model = T5ForConditionalGeneration.from_pretrained('t5-base')

# Translation
input_text = "translate English to German: Hello, how are you?"
input_ids = tokenizer(input_text, return_tensors='pt').input_ids
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
# Output: "Hallo, wie geht es dir?"

# Summarization
input_text = "summarize: The quick brown fox jumped over the lazy dog. It was a sunny day."
input_ids = tokenizer(input_text, return_tensors='pt').input_ids
outputs = model.generate(input_ids, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Comparison with BERT and GPT

| Aspect | BERT | GPT | T5 |
|--------|------|-----|-----|
| Architecture | Encoder | Decoder | Encoder-Decoder |
| Pre-training | MLM | Autoregressive | Span corruption |
| Task format | Classification head | Text completion | Text-to-text |
| Generation | No | Yes | Yes |
| Understanding | Strong | Via prompting | Strong |

## T5 Variants

| Model | Changes |
|-------|---------|
| T5 | Original encoder-decoder |
| mT5 | Multilingual (101 languages) |
| ByT5 | Byte-level (no tokenizer) |
| Flan-T5 | Instruction-tuned |
| UL2 | Mixture of denoising objectives |

## Ablation Findings

The T5 paper systematically studied:

| Component | Best Choice |
|-----------|-------------|
| Architecture | Encoder-decoder (vs decoder-only) |
| Pre-training | Span corruption |
| Span length | Mean length 3 |
| Corruption rate | 15% |
| Training mix | Multi-task with temperature |

## Summary

T5's contributions:

1. **Unified text-to-text framework**: All tasks as sequence-to-sequence
2. **Systematic study**: Ablation of architectures, objectives, training
3. **Encoder-decoder at scale**: Showed competitive with decoder-only
4. **Span corruption**: Efficient pre-training objective

T5 demonstrated that the encoder-decoder architecture remains powerful for NLP, especially for tasks with clear input-output structure like translation and summarization.

## References

- Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (2020)
- Xue et al., "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer" (2021)
- Chung et al., "Scaling Instruction-Finetuned Language Models" (Flan-T5, 2022)
