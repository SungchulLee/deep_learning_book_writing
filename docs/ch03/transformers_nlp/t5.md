# T5: Text-to-Text Transfer Transformer

## Introduction

T5 (Text-to-Text Transfer Transformer) unifies all NLP tasks into a single text-to-text format. Instead of task-specific architectures, every task is framed as generating output text from input text, making the model remarkably versatile.

## The Text-to-Text Framework

T5's key insight is that all NLP tasks can be expressed as text generation:

| Task | Input | Output |
|------|-------|--------|
| Translation | `translate English to German: That is good.` | `Das ist gut.` |
| Summarization | `summarize: <article>` | `<summary>` |
| Classification | `sentiment: This movie is great!` | `positive` |
| QA | `question: Who wrote Harry Potter? context: ...` | `J.K. Rowling` |

This unified format enables:
- Single model for all tasks
- Transfer learning across tasks
- Simple fine-tuning procedure

## Architecture

T5 uses the original encoder-decoder Transformer architecture:

$$
\text{T5} = \text{Encoder} + \text{Decoder}
$$

### Key Design Choices

| Aspect | T5 Choice | Alternative |
|--------|-----------|-------------|
| Architecture | Encoder-Decoder | Decoder-only (GPT) |
| Attention | Relative position bias | Absolute positional encoding |
| Normalization | Pre-norm | Post-norm |
| Activation | GELU (later ReLU variants) | ReLU |
| Embedding | Shared across encoder/decoder | Separate |

### Model Sizes

| Model | Encoder/Decoder Layers | $d_{\text{model}}$ | Parameters |
|-------|------------------------|---------|------------|
| T5-Small | 6/6 | 512 | 60M |
| T5-Base | 12/12 | 768 | 220M |
| T5-Large | 24/24 | 1024 | 770M |
| T5-3B | 24/24 | 1024 | 3B |
| T5-11B | 24/24 | 1024 | 11B |

## Pre-training: Span Corruption

T5 uses a denoising objective called **span corruption**:

1. Randomly select 15% of tokens
2. Replace consecutive spans with sentinel tokens `<extra_id_0>`, `<extra_id_1>`, etc.
3. Train the model to reconstruct the original spans

**Example:**

```
Original: "Thank you for inviting me to your party last week."
Input: "Thank you <extra_id_0> to your party <extra_id_1> week."
Target: "<extra_id_0> for inviting me <extra_id_1> last"
```

This is more efficient than BERT's token-level masking because multiple tokens are predicted per sentinel.

## PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, Tuple, List


class T5Config:
    """T5 model configuration."""
    
    def __init__(
        self,
        vocab_size: int = 32128,
        d_model: int = 512,
        d_kv: int = 64,
        d_ff: int = 2048,
        num_layers: int = 6,
        num_decoder_layers: int = None,
        num_heads: int = 8,
        relative_attention_num_buckets: int = 32,
        relative_attention_max_distance: int = 128,
        dropout_rate: float = 0.1,
        layer_norm_epsilon: float = 1e-6,
        feed_forward_proj: str = "relu"
    ):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.d_kv = d_kv
        self.d_ff = d_ff
        self.num_layers = num_layers
        self.num_decoder_layers = num_decoder_layers or num_layers
        self.num_heads = num_heads
        self.relative_attention_num_buckets = relative_attention_num_buckets
        self.relative_attention_max_distance = relative_attention_max_distance
        self.dropout_rate = dropout_rate
        self.layer_norm_epsilon = layer_norm_epsilon
        self.feed_forward_proj = feed_forward_proj


class T5RelativePositionBias(nn.Module):
    """
    T5-style relative position bias.
    
    Uses bucketed relative positions for efficiency.
    """
    
    def __init__(
        self,
        num_heads: int,
        num_buckets: int = 32,
        max_distance: int = 128,
        is_decoder: bool = False
    ):
        super().__init__()
        
        self.num_heads = num_heads
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        self.is_decoder = is_decoder
        
        self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)
    
    @staticmethod
    def _relative_position_bucket(
        relative_position: torch.Tensor,
        bidirectional: bool = True,
        num_buckets: int = 32,
        max_distance: int = 128
    ) -> torch.Tensor:
        """
        Compute relative position buckets.
        
        Buckets are logarithmically spaced for distant positions.
        """
        ret = 0
        n = -relative_position
        
        if bidirectional:
            num_buckets //= 2
            ret += (n < 0).to(torch.long) * num_buckets
            n = torch.abs(n)
        else:
            n = torch.max(n, torch.zeros_like(n))
        
        # Half buckets are for exact positions
        max_exact = num_buckets // 2
        is_small = n < max_exact
        
        # Other half are for logarithmically bigger bins
        val_if_large = max_exact + (
            torch.log(n.float() / max_exact) /
            math.log(max_distance / max_exact) *
            (num_buckets - max_exact)
        ).to(torch.long)
        
        val_if_large = torch.min(
            val_if_large,
            torch.full_like(val_if_large, num_buckets - 1)
        )
        
        ret += torch.where(is_small, n, val_if_large)
        return ret
    
    def forward(self, query_length: int, key_length: int) -> torch.Tensor:
        """
        Compute relative position bias.
        
        Returns: [1, num_heads, query_length, key_length]
        """
        device = self.relative_attention_bias.weight.device
        
        # Relative positions
        context_position = torch.arange(query_length, device=device)[:, None]
        memory_position = torch.arange(key_length, device=device)[None, :]
        relative_position = memory_position - context_position
        
        # Bucket the positions
        relative_position_bucket = self._relative_position_bucket(
            relative_position,
            bidirectional=not self.is_decoder,
            num_buckets=self.num_buckets,
            max_distance=self.max_distance
        )
        
        # Get bias values
        values = self.relative_attention_bias(relative_position_bucket)
        values = values.permute([2, 0, 1]).unsqueeze(0)
        
        return values


class T5Attention(nn.Module):
    """T5 Multi-Head Attention with relative position bias."""
    
    def __init__(
        self,
        config: T5Config,
        is_decoder: bool = False,
        has_relative_attention_bias: bool = False
    ):
        super().__init__()
        
        self.is_decoder = is_decoder
        self.has_relative_attention_bias = has_relative_attention_bias
        
        self.d_model = config.d_model
        self.d_kv = config.d_kv
        self.num_heads = config.num_heads
        self.inner_dim = self.num_heads * self.d_kv
        
        # Projections
        self.q = nn.Linear(config.d_model, self.inner_dim, bias=False)
        self.k = nn.Linear(config.d_model, self.inner_dim, bias=False)
        self.v = nn.Linear(config.d_model, self.inner_dim, bias=False)
        self.o = nn.Linear(self.inner_dim, config.d_model, bias=False)
        
        # Relative position bias
        if has_relative_attention_bias:
            self.relative_attention_bias = T5RelativePositionBias(
                num_heads=config.num_heads,
                num_buckets=config.relative_attention_num_buckets,
                max_distance=config.relative_attention_max_distance,
                is_decoder=is_decoder
            )
        
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = None,
        position_bias: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """
        Forward pass.
        
        Args:
            hidden_states: [batch, seq_len, d_model]
            key_value_states: For cross-attention, encoder hidden states
            position_bias: Pre-computed position bias
            attention_mask: Attention mask
            past_key_value: Cached KV for incremental decoding
            use_cache: Whether to return updated cache
        """
        batch_size, seq_len = hidden_states.shape[:2]
        
        # Self-attention or cross-attention
        is_cross_attention = key_value_states is not None
        
        # Compute Q
        query_states = self.q(hidden_states)
        query_states = query_states.view(batch_size, -1, self.num_heads, self.d_kv).transpose(1, 2)
        
        # Compute K, V (from encoder for cross-attention)
        if is_cross_attention:
            key_states = self.k(key_value_states)
            value_states = self.v(key_value_states)
        else:
            key_states = self.k(hidden_states)
            value_states = self.v(hidden_states)
        
        key_states = key_states.view(batch_size, -1, self.num_heads, self.d_kv).transpose(1, 2)
        value_states = value_states.view(batch_size, -1, self.num_heads, self.d_kv).transpose(1, 2)
        
        # Handle caching
        if past_key_value is not None:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        
        present_key_value = (key_states, value_states) if use_cache else None
        
        # Attention scores
        scores = torch.matmul(query_states, key_states.transpose(-2, -1))
        
        # Add position bias
        if position_bias is None and self.has_relative_attention_bias:
            position_bias = self.relative_attention_bias(
                query_length=seq_len,
                key_length=key_states.size(2)
            )
        
        if position_bias is not None:
            scores += position_bias
        
        # Apply attention mask
        if attention_mask is not None:
            scores += attention_mask
        
        # Softmax and dropout
        attn_weights = F.softmax(scores.float(), dim=-1).type_as(scores)
        attn_weights = self.dropout(attn_weights)
        
        # Apply to values
        attn_output = torch.matmul(attn_weights, value_states)
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, -1, self.inner_dim)
        attn_output = self.o(attn_output)
        
        return attn_output, position_bias, present_key_value


class T5LayerFF(nn.Module):
    """T5 Feed-Forward Layer."""
    
    def __init__(self, config: T5Config):
        super().__init__()
        
        self.DenseReluDense = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff, bias=False),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.d_ff, config.d_model, bias=False)
        )
        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """Pre-norm FFN."""
        normed = self.layer_norm(hidden_states)
        ff_output = self.DenseReluDense(normed)
        return hidden_states + self.dropout(ff_output)


class T5EncoderBlock(nn.Module):
    """T5 Encoder Block."""
    
    def __init__(self, config: T5Config, has_relative_attention_bias: bool = False):
        super().__init__()
        
        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.self_attention = T5Attention(
            config,
            is_decoder=False,
            has_relative_attention_bias=has_relative_attention_bias
        )
        self.feed_forward = T5LayerFF(config)
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_bias: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Forward pass."""
        # Self-attention
        normed = self.layer_norm(hidden_states)
        attn_output, position_bias, _ = self.self_attention(
            normed,
            position_bias=position_bias,
            attention_mask=attention_mask
        )
        hidden_states = hidden_states + self.dropout(attn_output)
        
        # FFN
        hidden_states = self.feed_forward(hidden_states)
        
        return hidden_states, position_bias


class T5DecoderBlock(nn.Module):
    """T5 Decoder Block with self-attention and cross-attention."""
    
    def __init__(self, config: T5Config, has_relative_attention_bias: bool = False):
        super().__init__()
        
        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        
        # Masked self-attention
        self.self_attention = T5Attention(
            config,
            is_decoder=True,
            has_relative_attention_bias=has_relative_attention_bias
        )
        
        # Cross-attention
        self.cross_attention_layer_norm = nn.LayerNorm(
            config.d_model, eps=config.layer_norm_epsilon
        )
        self.cross_attention = T5Attention(config, is_decoder=True)
        
        # FFN
        self.feed_forward = T5LayerFF(config)
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        position_bias: Optional[torch.Tensor] = None,
        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple] = None,
        use_cache: bool = False
    ) -> Tuple:
        """Forward pass."""
        self_attn_past_key_value = past_key_value[:2] if past_key_value else None
        cross_attn_past_key_value = past_key_value[2:] if past_key_value else None
        
        # Self-attention
        normed = self.layer_norm(hidden_states)
        attn_output, position_bias, present_key_value = self.self_attention(
            normed,
            position_bias=position_bias,
            attention_mask=attention_mask,
            past_key_value=self_attn_past_key_value,
            use_cache=use_cache
        )
        hidden_states = hidden_states + self.dropout(attn_output)
        
        # Cross-attention
        normed = self.cross_attention_layer_norm(hidden_states)
        cross_attn_output, _, cross_present = self.cross_attention(
            normed,
            key_value_states=encoder_hidden_states,
            attention_mask=encoder_attention_mask,
            past_key_value=cross_attn_past_key_value,
            use_cache=use_cache
        )
        hidden_states = hidden_states + self.dropout(cross_attn_output)
        
        # FFN
        hidden_states = self.feed_forward(hidden_states)
        
        present = present_key_value + cross_present if use_cache else None
        
        return hidden_states, position_bias, present


class T5Model(nn.Module):
    """Complete T5 Encoder-Decoder Model."""
    
    def __init__(self, config: T5Config):
        super().__init__()
        self.config = config
        
        # Shared embeddings
        self.shared = nn.Embedding(config.vocab_size, config.d_model)
        
        # Encoder
        self.encoder_embed_dropout = nn.Dropout(config.dropout_rate)
        self.encoder_blocks = nn.ModuleList([
            T5EncoderBlock(config, has_relative_attention_bias=(i == 0))
            for i in range(config.num_layers)
        ])
        self.encoder_final_layer_norm = nn.LayerNorm(
            config.d_model, eps=config.layer_norm_epsilon
        )
        
        # Decoder
        self.decoder_embed_dropout = nn.Dropout(config.dropout_rate)
        self.decoder_blocks = nn.ModuleList([
            T5DecoderBlock(config, has_relative_attention_bias=(i == 0))
            for i in range(config.num_decoder_layers)
        ])
        self.decoder_final_layer_norm = nn.LayerNorm(
            config.d_model, eps=config.layer_norm_epsilon
        )
        
        # LM head
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
    
    def encode(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode input sequence."""
        hidden_states = self.shared(input_ids)
        hidden_states = self.encoder_embed_dropout(hidden_states)
        
        # Convert attention mask
        if attention_mask is not None:
            extended_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * -1e9
        else:
            extended_mask = None
        
        position_bias = None
        for block in self.encoder_blocks:
            hidden_states, position_bias = block(
                hidden_states,
                attention_mask=extended_mask,
                position_bias=position_bias
            )
        
        hidden_states = self.encoder_final_layer_norm(hidden_states)
        return hidden_states
    
    def decode(
        self,
        decoder_input_ids: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[List]]:
        """Decode output sequence."""
        hidden_states = self.shared(decoder_input_ids)
        hidden_states = self.decoder_embed_dropout(hidden_states)
        
        # Causal mask for decoder
        seq_len = decoder_input_ids.size(1)
        causal_mask = torch.triu(
            torch.ones(seq_len, seq_len, device=decoder_input_ids.device),
            diagonal=1
        ).bool()
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) * -1e9
        
        # Encoder attention mask
        if encoder_attention_mask is not None:
            encoder_extended_mask = (
                1.0 - encoder_attention_mask.unsqueeze(1).unsqueeze(2)
            ) * -1e9
        else:
            encoder_extended_mask = None
        
        position_bias = None
        presents = [] if use_cache else None
        
        for i, block in enumerate(self.decoder_blocks):
            past = past_key_values[i] if past_key_values else None
            
            hidden_states, position_bias, present = block(
                hidden_states,
                encoder_hidden_states,
                attention_mask=causal_mask,
                encoder_attention_mask=encoder_extended_mask,
                position_bias=position_bias,
                past_key_value=past,
                use_cache=use_cache
            )
            
            if use_cache:
                presents.append(present)
        
        hidden_states = self.decoder_final_layer_norm(hidden_states)
        return hidden_states, presents
    
    def forward(
        self,
        input_ids: torch.Tensor,
        decoder_input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Forward pass."""
        # Encode
        encoder_hidden_states = self.encode(input_ids, attention_mask)
        
        # Decode
        decoder_hidden_states, _ = self.decode(
            decoder_input_ids,
            encoder_hidden_states,
            encoder_attention_mask=attention_mask
        )
        
        # LM head
        logits = self.lm_head(decoder_hidden_states)
        
        # Compute loss
        loss = None
        if labels is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
        
        return {'loss': loss, 'logits': logits}


# Example usage
if __name__ == "__main__":
    config = T5Config(vocab_size=32128, d_model=512, num_layers=6, num_heads=8)
    model = T5Model(config)
    
    # Sample input
    input_ids = torch.randint(0, config.vocab_size, (2, 32))
    decoder_input_ids = torch.randint(0, config.vocab_size, (2, 16))
    labels = decoder_input_ids.clone()
    
    outputs = model(input_ids, decoder_input_ids, labels=labels)
    
    print(f"Logits shape: {outputs['logits'].shape}")
    print(f"Loss: {outputs['loss'].item():.4f}")
    print(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")
```

## Task Prefixes

T5 uses text prefixes to specify tasks:

```python
TASK_PREFIXES = {
    "translation_en_to_de": "translate English to German: ",
    "translation_en_to_fr": "translate English to French: ",
    "summarization": "summarize: ",
    "sentiment": "sentiment: ",
    "question": "question: ",
    "answer_span": "answer: ",
}
```

## T5 Variants

| Variant | Key Changes |
|---------|-------------|
| **T5 v1.1** | No dropout during pre-training, GEGLU activation |
| **mT5** | Multilingual, 101 languages |
| **Flan-T5** | Instruction-tuned on 1800+ tasks |
| **UL2** | Mixture of Denoisers objective |

## Summary

T5's unified text-to-text framework:

1. **Simplicity**: One model for all NLP tasks
2. **Scalability**: Works from 60M to 11B parameters
3. **Transfer Learning**: Pre-training transfers across tasks
4. **Flexibility**: Easy to add new tasks via prompts

## References

1. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with T5." JMLR.
2. Chung, H., et al. (2022). "Scaling Instruction-Finetuned Language Models."
3. Tay, Y., et al. (2022). "UL2: Unifying Language Learning Paradigms."
