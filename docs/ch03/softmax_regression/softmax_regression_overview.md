# PyTorch Softmax Regression Tutorial Series

A comprehensive, progressive tutorial series on softmax regression and multi-class classification using PyTorch. Designed for undergraduates, this series takes you from fundamental concepts to advanced techniques with fully commented code.

## üìö Overview

This tutorial series provides a hands-on learning experience for understanding and implementing softmax regression (multinomial logistic regression) in PyTorch. Each level builds upon the previous one, progressively increasing in complexity and introducing new concepts.

**Total Learning Time:** 4-6 hours  
**Prerequisites:** Basic Python, basic calculus and linear algebra  
**Framework:** PyTorch  

---

## üéØ Learning Objectives

By completing this tutorial series, you will:

- ‚úÖ Understand the mathematics behind softmax and cross-entropy
- ‚úÖ Build neural networks for multi-class classification
- ‚úÖ Train models on real-world datasets (MNIST, Fashion-MNIST, CIFAR-10)
- ‚úÖ Implement advanced techniques (regularization, normalization, scheduling)
- ‚úÖ Create reusable training pipelines
- ‚úÖ Conduct systematic experiments and comparisons

---

## üìñ Tutorial Structure

### Level 1: Softmax Regression Fundamentals
**File:** `01_fundamentals.py`  
**Difficulty:** ‚≠ê Beginner  
**Time:** 20-30 minutes  

**What You'll Learn:**
- What is softmax and why we use it
- How cross-entropy loss works
- The relationship between logits, probabilities, and loss
- PyTorch's `nn.CrossEntropyLoss` and how to use it correctly
- Common mistakes and best practices

**Key Concepts:**
```python
# Softmax converts logits to probabilities
logits = [2.0, 1.0, 0.1]
probabilities = softmax(logits)  # [0.65, 0.25, 0.10]

# CrossEntropyLoss combines softmax + log + NLL
criterion = nn.CrossEntropyLoss()
loss = criterion(logits, target_class)  # Input: logits (NOT probabilities!)
```

**Topics Covered:**
- Softmax function implementation
- Cross-entropy loss calculation
- NumPy vs PyTorch implementations
- Batch processing
- One-hot vs class indices

---

### Level 2: Building Your First Softmax Classifier
**File:** `02_simple_classifier.py`  
**Difficulty:** ‚≠ê‚≠ê Beginner-Intermediate  
**Time:** 30-45 minutes  

**What You'll Learn:**
- Build a simple feedforward neural network
- Implement the complete training loop
- Visualize decision boundaries
- Evaluate model performance
- Save and load trained models

**Model Architecture:**
```
Input (2D features)
    ‚Üì
Hidden Layer (64 neurons) + ReLU
    ‚Üì
Hidden Layer (32 neurons) + ReLU
    ‚Üì
Output Layer (3 classes)
```

**Training Loop Structure:**
```python
for epoch in range(num_epochs):
    # 1. Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # 2. Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 3. Evaluate
    validate(model, val_data)
```

**Topics Covered:**
- Neural network architecture design
- Training and validation loops
- Metrics tracking (loss, accuracy)
- Decision boundary visualization
- Model persistence

---

### Level 3: Softmax Regression on MNIST Dataset
**File:** `03_mnist.py`  
**Difficulty:** ‚≠ê‚≠ê‚≠ê Intermediate  
**Time:** 45-60 minutes  

**What You'll Learn:**
- Work with real image datasets (MNIST)
- Use PyTorch DataLoaders for efficient batching
- Implement train/validation/test splits
- Handle mini-batch training
- Generate confusion matrices
- Analyze per-class performance

**Dataset Information:**
- **MNIST:** 70,000 handwritten digits (0-9)
  - Training: 60,000 images
  - Test: 10,000 images
  - Image size: 28√ó28 grayscale
  - 10 classes

**Model Features:**
- Flatten layer for image data
- Dropout for regularization
- Batch normalization
- Multiple hidden layers

**Expected Performance:**
- Training accuracy: ~98%
- Test accuracy: ~97%
- Training time: ~2-3 minutes (CPU)

**Topics Covered:**
- Image data preprocessing
- DataLoader usage and batching
- Dropout regularization
- Confusion matrix analysis
- Model checkpointing

---

### Level 4: Advanced Softmax Regression Techniques
**File:** `04_advanced.py`  
**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê Intermediate-Advanced  
**Time:** 60-90 minutes  

**What You'll Learn:**
- Implement softmax regression from scratch (NumPy)
- Advanced regularization techniques
- Learning rate scheduling strategies
- Early stopping implementation
- Gradient clipping
- Custom loss functions (label smoothing)

**Advanced Techniques:**

1. **From-Scratch Implementation**
   - Pure NumPy softmax regression
   - Manual gradient computation
   - L2 regularization

2. **Batch Normalization**
   - Normalize layer inputs
   - Improve training stability
   - Enable higher learning rates

3. **Learning Rate Scheduling**
   - Step decay
   - Exponential decay
   - Cosine annealing

4. **Early Stopping**
   - Monitor validation loss
   - Prevent overfitting
   - Save best model

5. **Gradient Clipping**
   - Prevent exploding gradients
   - Improve stability

6. **Label Smoothing**
   - Soft targets vs hard targets
   - Reduce overconfidence
   - Better generalization

**Topics Covered:**
- Manual backpropagation
- Regularization strategies
- Training optimization
- Advanced PyTorch features
- Custom training callbacks

---

### Level 5: Comprehensive Multi-Dataset Classification
**File:** `05_comprehensive.py`  
**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Advanced  
**Time:** 90-120 minutes  

**What You'll Learn:**
- Build modular, reusable ML pipelines
- Work with multiple datasets
- Systematic experiment tracking
- Compare architectures and hyperparameters
- Generate comprehensive reports

**Project Components:**

1. **DatasetManager**
   - Unified interface for multiple datasets
   - Support for MNIST, Fashion-MNIST, CIFAR-10
   - Automatic preprocessing and splitting
   - Configurable batch sizes

2. **ModelFactory**
   - Create models programmatically
   - Three architectures: Simple, Medium, Deep
   - Flexible hyperparameter configuration
   - Easy to extend

3. **Trainer**
   - Complete training pipeline
   - Automatic metric tracking
   - Best model selection
   - Early stopping support
   - Learning rate scheduling

4. **ExperimentRunner**
   - Run multiple experiments automatically
   - Fair comparison across configurations
   - Generate comparison reports
   - Track all hyperparameters

**Supported Datasets:**
- MNIST (handwritten digits)
- Fashion-MNIST (clothing items)
- CIFAR-10 (color images)

**Model Architectures:**
```
Simple:  Input ‚Üí 128 ‚Üí Output
Medium:  Input ‚Üí 256 ‚Üí 128 ‚Üí Output (with BatchNorm)
Deep:    Input ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí Output (with BatchNorm)
```

**Topics Covered:**
- Software engineering for ML
- Experiment design and tracking
- Systematic hyperparameter comparison
- Results visualization and reporting
- Production-ready code structure

---

## üöÄ Getting Started

### Prerequisites

Make sure you have the following installed:

```bash
Python 3.7+
PyTorch 1.9+
torchvision
numpy
matplotlib
scikit-learn
```

### Installation

1. **Clone or download this repository**

2. **Install dependencies:**
```bash
pip install torch torchvision numpy matplotlib scikit-learn
```

Or use the requirements file:
```bash
pip install -r requirements.txt
```

3. **Run any level:**
```bash
python 01_fundamentals.py
python 02_simple_classifier.py
python 03_mnist.py
python 04_advanced.py
python 05_comprehensive.py
```

---

## üìä Expected Results

### Level 3 (MNIST)
- **Test Accuracy:** ~97%
- **Training Time:** 2-3 minutes (CPU), <1 minute (GPU)

### Level 5 (Comprehensive)
| Dataset | Model | Test Accuracy |
|---------|-------|---------------|
| MNIST | Simple | ~96% |
| MNIST | Medium | ~98% |
| MNIST | Deep | ~98% |
| Fashion-MNIST | Simple | ~85% |
| Fashion-MNIST | Medium | ~88% |
| Fashion-MNIST | Deep | ~89% |
| CIFAR-10 | Simple | ~45% |
| CIFAR-10 | Medium | ~52% |
| CIFAR-10 | Deep | ~55% |

*Note: CIFAR-10 benefits more from convolutional architectures (CNNs) than fully-connected networks.*

---

## üí° Key Concepts Explained

### Softmax Function
Converts raw scores (logits) into probabilities:

```
softmax(z_i) = exp(z_i) / Œ£ exp(z_j)
```

**Properties:**
- Output values between 0 and 1
- Sum of outputs equals 1
- Differentiable (needed for backpropagation)

### Cross-Entropy Loss
Measures how well predicted probabilities match true distribution:

```
Loss = -Œ£ y_i * log(p_i)
```

For classification with one true class:
```
Loss = -log(p_true_class)
```

**Intuition:**
- Perfect prediction (p=1.0) ‚Üí Loss = 0
- Wrong prediction (p‚Üí0) ‚Üí Loss ‚Üí ‚àû

### Why CrossEntropyLoss Takes Logits

PyTorch's `nn.CrossEntropyLoss` expects **raw logits**, not probabilities:

```python
# ‚úÖ CORRECT
outputs = model(x)  # logits
loss = criterion(outputs, targets)

# ‚ùå WRONG
outputs = torch.softmax(model(x), dim=1)  # probabilities
loss = criterion(outputs, targets)  # Double softmax!
```

**Reason:** Combining operations is more numerically stable:
- Prevents underflow/overflow
- More accurate gradients
- Faster computation

---

## üéì Recommended Learning Path

### For Beginners:
1. Start with **Level 1** - Understand fundamentals
2. Complete **Level 2** - Build first classifier
3. Move to **Level 3** - Work with real data
4. Skip to **Level 5** - See production patterns

### For Intermediate Learners:
1. Review **Level 1** - Refresh concepts
2. Study **Level 3** - MNIST implementation
3. Deep dive **Level 4** - Advanced techniques
4. Master **Level 5** - Complete pipeline

### For Advanced Learners:
1. Skim **Level 1-2** - Quick review
2. Focus on **Level 4** - Advanced methods
3. Analyze **Level 5** - Software engineering
4. Extend with custom datasets/models

---

## üõ†Ô∏è Common Issues and Solutions

### Issue 1: CUDA Out of Memory
**Solution:** Reduce batch size
```python
train_loader = DataLoader(dataset, batch_size=64)  # Instead of 128
```

### Issue 2: Model Not Learning
**Checklist:**
- [ ] Using correct loss function (`nn.CrossEntropyLoss`)
- [ ] Targets are class indices, not one-hot
- [ ] Learning rate not too high/low
- [ ] Calling `optimizer.zero_grad()` before backward
- [ ] Calling `optimizer.step()` after backward

### Issue 3: High Training but Low Test Accuracy
**Overfitting!** Try:
- Increase dropout rate
- Add L2 regularization
- Use data augmentation
- Reduce model complexity
- Use early stopping

### Issue 4: Loss is NaN
**Causes:**
- Learning rate too high
- Numerical instability
- Missing gradient clipping

**Solutions:**
```python
# Lower learning rate
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Add gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## üìà Performance Tips

### Training Speed
1. **Use GPU if available:**
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
```

2. **Increase batch size:**
```python
train_loader = DataLoader(dataset, batch_size=256)  # Larger batches
```

3. **Use multiple workers:**
```python
train_loader = DataLoader(dataset, num_workers=4)
```

### Model Accuracy
1. **Try different architectures** (wider, deeper)
2. **Tune hyperparameters** (learning rate, dropout)
3. **Use batch normalization**
4. **Apply data augmentation**
5. **Ensemble multiple models**

---

## üî¨ Exercises and Extensions

### Beginner Exercises
1. Modify Level 2 to use different activation functions (LeakyReLU, ELU)
2. Change the number of hidden layers and neurons
3. Train on different synthetic datasets (moons, circles)

### Intermediate Exercises
1. Implement your own loss function
2. Add more datasets to Level 5 (CIFAR-100, SVHN)
3. Create visualizations of learned features
4. Implement learning rate warmup

### Advanced Exercises
1. Implement mixup or cutmix augmentation
2. Add attention mechanisms
3. Create a neural architecture search framework
4. Implement adversarial training
5. Add uncertainty estimation

---

## üìö Further Reading

### Theory
- **Deep Learning Book** - Ian Goodfellow et al.
  - Chapter 3: Probability and Information Theory
  - Chapter 6: Deep Feedforward Networks
  
- **Pattern Recognition and Machine Learning** - Christopher Bishop
  - Chapter 4: Linear Models for Classification

### PyTorch Resources
- [PyTorch Official Tutorials](https://pytorch.org/tutorials/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Dive into Deep Learning](https://d2l.ai/)

### Papers
- "ImageNet Classification with Deep Convolutional Neural Networks" (AlexNet)
- "Batch Normalization: Accelerating Deep Network Training"
- "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"

---

## ü§ù Contributing

Improvements and extensions are welcome! Areas for contribution:

- Additional datasets
- New model architectures
- More advanced techniques
- Better visualizations
- Documentation improvements
- Bug fixes

---

## üìù License

This tutorial series is provided for educational purposes. Feel free to use, modify, and distribute with attribution.

---

## üë®‚Äçüè´ About

This comprehensive tutorial series was designed to provide a practical, hands-on learning experience for understanding softmax regression and multi-class classification in PyTorch. The progressive structure ensures learners build a solid foundation before tackling advanced concepts.

**Target Audience:** Undergraduate students, self-learners, and anyone interested in deep learning

**Learning Philosophy:** Learn by doing, with fully commented code and detailed explanations at every step.

---

## üéâ Acknowledgments

Concepts and techniques in this tutorial are based on:
- PyTorch official documentation and tutorials
- Stanford CS231n course materials
- Fast.ai deep learning course
- Various PyTorch community resources

---

## üìû Support

If you encounter issues or have questions:

1. Check the "Common Issues and Solutions" section
2. Review the comments in the code files
3. Ensure all dependencies are correctly installed
4. Try running with default parameters first

---

## üó∫Ô∏è Learning Roadmap

```
Level 1: Fundamentals (20-30 min)
    ‚Üì
Level 2: Simple Classifier (30-45 min)
    ‚Üì
Level 3: MNIST Dataset (45-60 min)
    ‚Üì
Level 4: Advanced Techniques (60-90 min)
    ‚Üì
Level 5: Comprehensive Project (90-120 min)
    ‚Üì
Master of Softmax Regression! üéì
```

**Total Time Investment:** 4-6 hours  
**Skill Level Gained:** Intermediate to Advanced

---

## ‚ú® What's Next?

After completing this tutorial series, you're ready to:

1. **Explore Convolutional Neural Networks (CNNs)**
   - Better for image data
   - More efficient than fully-connected networks

2. **Learn Recurrent Neural Networks (RNNs)**
   - For sequential data
   - Text and time series

3. **Study Advanced Architectures**
   - ResNet, DenseNet
   - Transformers
   - Vision Transformers

4. **Apply to Real Projects**
   - Kaggle competitions
   - Research problems
   - Industry applications

5. **Explore Specialized Topics**
   - Transfer learning
   - Few-shot learning
   - Meta-learning

---

**Happy Learning! üöÄ**

*Remember: The best way to learn is by doing. Run the code, experiment with parameters, break things, and understand why they break!*
