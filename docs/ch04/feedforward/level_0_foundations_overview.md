# Level 0: Mathematical Foundations

## üéØ Purpose

Before diving into PyTorch, it's crucial to understand what's happening "under the hood". This level builds your intuition by implementing neural network concepts from scratch using only NumPy.

## üìö Why Start Here?

1. **Understand the Math**: See exactly how gradients are computed
2. **Appreciate PyTorch**: You'll understand why frameworks are so powerful
3. **Debug Better**: Know what can go wrong and why
4. **Build Intuition**: Visualize how learning actually happens

## üìñ Files in This Level

### 01_linear_regression_numpy.py ‚≠ê
**Difficulty**: Beginner | **Time**: 45-60 min

Your first neural network - implemented with pure NumPy!

**What You'll Learn**:
- Forward pass computation (making predictions)
- Loss function calculation (measuring error)
- Gradient computation (finding the direction to improve)
- Gradient descent (actually improving)

**Key Concepts**:
- `y = wx + b` (linear model)
- Mean Squared Error (MSE)
- Manual derivative calculation
- Parameter updates

**Why It Matters**: This is the foundation of ALL neural networks. Every complex deep learning model does this same process, just with more layers.

---

### 02_linear_regression_pytorch.py ‚≠ê‚≠ê
**Difficulty**: Beginner | **Time**: 30-45 min

The SAME problem as file 01, but now using PyTorch tensors and autograd.

**What You'll Learn**:
- PyTorch tensors (like NumPy arrays, but smarter)
- Automatic differentiation (no manual gradient math!)
- torch.optim (optimizers handle updates for you)
- The power of frameworks

**Key Comparison**:
| Aspect | NumPy (File 01) | PyTorch (File 02) |
|--------|----------------|-------------------|
| Gradients | Manual calculation | Automatic with `.backward()` |
| Updates | Manual `w = w - lr * dw` | `optimizer.step()` |
| GPU | Not supported | Just add `.cuda()` |

**Why It Matters**: See how much PyTorch simplifies your life! This motivates learning the framework.

---

### 03_simple_nn_manual.py ‚≠ê‚≠ê‚≠ê
**Difficulty**: Intermediate | **Time**: 60-90 min

A complete multi-layer neural network implemented from scratch!

**What You'll Learn**:
- Forward pass through multiple layers
- Backpropagation through the network
- Non-linear activation functions (ReLU, Sigmoid)
- Why depth matters

**Key Concepts**:
- Hidden layers and what they do
- Chain rule in backpropagation
- Activation functions and their derivatives
- Matrix multiplication for batch processing

**Architecture**:
```
Input (2) ‚Üí Hidden (4) ‚Üí Hidden (4) ‚Üí Output (1)
          ReLU         ReLU         Sigmoid
```

**Why It Matters**: This is a REAL neural network. You'll see how information flows forward and gradients flow backward through multiple layers.

## üéì Learning Path

```
01_linear_regression_numpy.py (Required)
    ‚Üì
    Learn: Basic gradient descent
    ‚Üì
02_linear_regression_pytorch.py (Required)
    ‚Üì
    Learn: PyTorch fundamentals
    ‚Üì
03_simple_nn_manual.py (Recommended)
    ‚Üì
    Learn: Multi-layer networks
    ‚Üì
Ready for Level 1! üéâ
```

## üí° Study Tips

### For File 01:
- Draw the computational graph on paper
- Calculate gradients by hand for one example
- Try different learning rates (0.1, 0.01, 0.001)
- Plot the loss curve - it should go down!

### For File 02:
- Compare code side-by-side with File 01
- Print gradients to verify they match your intuition
- Time both implementations (PyTorch is faster!)

### For File 03:
- Trace the forward pass for one example
- Understand why we need non-linear activations
- Try removing ReLU - see what happens!
- Experiment with different network widths/depths

## üîß Common Struggles & Solutions

**"The math is confusing!"**
‚Üí Focus on the shapes. If input is (100, 1), what should weight shape be?

**"Why do we need derivatives?"**
‚Üí They tell us which direction to move weights to reduce error.

**"Backpropagation seems magical"**
‚Üí It's just chain rule from calculus. We work backwards through the graph.

**"Should I memorize the derivative formulas?"**
‚Üí No! Understand the concept. PyTorch will compute them for you.

## ‚úÖ Level 0 Completion Checklist

Before moving to Level 1, make sure you can:

- [ ] Explain what a forward pass does
- [ ] Describe what a loss function measures
- [ ] Understand why we compute gradients
- [ ] Implement gradient descent from scratch
- [ ] Explain what `.backward()` does in PyTorch
- [ ] Describe the role of activation functions
- [ ] Draw a simple neural network architecture

## üéØ Next Level Preview

**Level 1: PyTorch Basics** will teach you:
- PyTorch's automatic differentiation in depth
- The `nn.Module` pattern for building models
- Different optimizers (Adam, SGD, RMSprop)
- How to structure training loops properly

---

**Time to start!** Open `01_linear_regression_numpy.py` and begin your journey! üöÄ

Remember: Type the code yourself, don't copy-paste. Learning happens in your fingers, not your eyes!
