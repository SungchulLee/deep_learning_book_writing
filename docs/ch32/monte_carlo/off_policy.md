# 32.5.3 Off-Policy Monte Carlo

## Motivation

**On-policy** methods learn about the policy being executed. But what if we want to learn about one policy (the **target policy** $\pi$) while following a different policy (the **behavior policy** $b$)?

This is the **off-policy** setting, which enables:

1. Learning the optimal policy while following an exploratory policy
2. Learning from data generated by other agents or historical data
3. Reusing experience across different policies

## Off-Policy Prediction

Given episodes generated by behavior policy $b$, estimate $V_\pi$ or $Q_\pi$.

### Requirements

**Coverage assumption**: Every action taken by $\pi$ must also be possible under $b$:

$$\pi(a|s) > 0 \implies b(a|s) > 0 \quad \text{for all } s, a$$

This ensures all relevant state-action pairs are visited under $b$.

## Off-Policy MC with Importance Sampling

The expected return under $\pi$ can be estimated from episodes under $b$ using **importance sampling ratios**:

$$\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(A_k | S_k)}{b(A_k | S_k)}$$

This ratio reweights returns to account for the difference between $b$ and $\pi$.

### Off-Policy MC Prediction

$$V_\pi(s) \approx \frac{\sum_{\text{episodes}} \rho_{t:T-1} G_t}{\sum_{\text{episodes}} 1} \quad \text{(ordinary IS)}$$

$$V_\pi(s) \approx \frac{\sum_{\text{episodes}} \rho_{t:T-1} G_t}{\sum_{\text{episodes}} \rho_{t:T-1}} \quad \text{(weighted IS)}$$

## Off-Policy MC Control

```
Initialize: Q(s,a) arbitrary, C(s,a) = 0, π = greedy w.r.t. Q

For each episode generated by b:
    G = 0
    W = 1
    For t = T-1, T-2, ..., 0:
        G = γG + R_{t+1}
        C(S_t, A_t) += W
        Q(S_t, A_t) += (W / C(S_t, A_t)) * (G - Q(S_t, A_t))
        π(S_t) = argmax_a Q(S_t, a)
        If A_t ≠ π(S_t): break     # Importance sampling terminates
        W = W * (1 / b(A_t | S_t))
```

Key insight: When the behavior policy takes an action that the target policy wouldn't, the importance weight becomes zero and we can stop processing that episode early.

## Challenges

1. **High variance**: Importance sampling ratios can be very large, leading to unstable estimates
2. **Early termination**: If $b$ and $\pi$ differ significantly, many episodes contribute zero or very little information
3. **Sample inefficiency**: Much data may be wasted when policies are very different
4. **Coverage**: $b$ must cover all actions $\pi$ might take

## Financial Relevance

Off-policy learning is crucial in finance:

- **Historical data**: Learn optimal strategies from data collected under a different (possibly suboptimal) historical policy
- **Paper trading**: Test aggressive strategies using data from conservative trading
- **Regulatory constraints**: The behavior policy may include safety constraints; learn unconstrained optimal policy for analysis
- **Strategy comparison**: Evaluate multiple strategies from a single dataset

## Summary

Off-policy MC methods enable learning about one policy from data generated by another, using importance sampling to correct for the distribution mismatch. While powerful and theoretically sound, high variance of importance sampling ratios is a practical challenge that motivates the techniques discussed in the next section.
